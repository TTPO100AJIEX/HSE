{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJcJtA0OF3CP"
      },
      "source": [
        "## Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Implement vanilla autoencoder\n",
        "* Train it on MNIST dataset MNIST\n",
        "* Display digits recovered dy AE\n",
        "* Display distribution of embeddings in latent space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ideas for extra work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Find the best latent space size\n",
        "* Implement noise filtration with AE\n",
        "* Test vector arithmetic in laent space\n",
        "* Implemet VAE\n",
        "    * Use Autoencoder class as base class\n",
        "    * Implement VAE Loss class\n",
        "    * Plot embeddings manifold in VAE latent space\n",
        "    * Compare decoding results VAE latent space with vanilla Autoencoder results\n",
        "* Replace reconstruction loss from MSE to BCE\n",
        "* Implement Conditional Autoencoder or CVAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import abc\n",
        "import typing\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import tqdm\n",
        "import torch\n",
        "import wandb\n",
        "import numpy\n",
        "import seaborn\n",
        "import torchvision\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.utils.data as torchdata\n",
        "from torchvision.transforms import v2 as transforms\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "device = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(device)\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "def set_random_seed(seed):\n",
        "    random.seed(seed)\n",
        "    numpy.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "def fix_random():\n",
        "    return set_random_seed(RANDOM_STATE)\n",
        "fix_random()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1RSfPrxFzVN"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "http://yann.lecun.com/exdb/mnist/\n",
        "\n",
        "The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples.\n",
        "\n",
        "The images were centered in a 28x28 image by computing the center of mass of the pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.v2.Compose([\n",
        "    torchvision.transforms.v2.Resize((32, 32)), # Added Resize for convenience later\n",
        "    torchvision.transforms.v2.ToImage(),\n",
        "    torchvision.transforms.v2.ToDtype(torch.float32, scale = True)\n",
        "])\n",
        "\n",
        "def calc_channel_stats(dataset: torchdata.Dataset) -> typing.Tuple[torch.Tensor, torch.Tensor]:\n",
        "    all_images = torch.stack([ item[0] for item in dataset ])\n",
        "    mean = torch.mean(all_images, dim = [0, 2, 3])\n",
        "    std = torch.std(all_images, dim = [0, 2, 3])\n",
        "    return mean, std\n",
        "\n",
        "mean, std = calc_channel_stats(torchvision.datasets.MNIST('mnist', train = True, download = True, transform = transform))\n",
        "print(mean, std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvPsg0rJFrMf"
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.v2.Compose([\n",
        "    torchvision.transforms.v2.Resize((32, 32)),\n",
        "    torchvision.transforms.v2.ToImage(),\n",
        "    torchvision.transforms.v2.ToDtype(torch.float32, scale = True),\n",
        "    torchvision.transforms.v2.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "train_ds = torchvision.datasets.MNIST('mnist', train = True, download = True, transform = transform)\n",
        "test_ds = torchvision.datasets.MNIST('mnist', train = False, download = True, transform = transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4H_0snrb3tr"
      },
      "source": [
        "Display some samples along with corresponding labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Denormalization\n",
        "denormalize = torchvision.transforms.v2.Compose([\n",
        "    torchvision.transforms.v2.Normalize(mean = torch.zeros_like(mean), std = 1 / std),\n",
        "    torchvision.transforms.v2.Normalize(mean = -mean, std = torch.ones_like(std))\n",
        "])\n",
        "\n",
        "def display_image(image: torch.Tensor, label: str):\n",
        "    plt.axis('off')\n",
        "    plt.title('{}'.format(label))\n",
        "    plt.imshow((torch.clamp(denormalize(image), 0., 1.).permute(1, 2, 0).detach().cpu().numpy() * 255).astype(numpy.uint8), cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_examples(dataset: torchdata.Dataset, row: int):\n",
        "    train_loader = torchdata.DataLoader(dataset, batch_size = 10, shuffle = True)\n",
        "    for i, (image, label) in enumerate(zip(*next(iter(train_loader)))):\n",
        "        plt.subplot(3, 10, i + 10 * (row - 1) + 1)\n",
        "        display_image(image, label)\n",
        "\n",
        "# Display some samples from each dataset\n",
        "fix_random()\n",
        "plt.rcParams[\"figure.figsize\"] = (15, 5)\n",
        "display_examples(train_ds, 1)\n",
        "display_examples(test_ds, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTwMOTRSksqp"
      },
      "source": [
        "Define dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = torchdata.DataLoader(train_ds, shuffle = True, batch_size = 128)\n",
        "test_loader = torchdata.DataLoader(test_ds, shuffle = False, batch_size = 256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEjkNhatIwbd"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseEncoder(torch.nn.Module, abc.ABC):\n",
        "    def __init__(self, latent_size: int):\n",
        "        super().__init__()\n",
        "        self.latent_size = latent_size\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(input)\n",
        "\n",
        "class BaseDecoder(torch.nn.Module, abc.ABC):\n",
        "    def __init__(self, latent_size: int):\n",
        "        super().__init__()\n",
        "        self.latent_size = latent_size\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(input)\n",
        "    \n",
        "class BaseAutoEncoder(torch.nn.Module, abc.ABC):\n",
        "    def __init__(self, latent_size: int, encoder_class = BaseEncoder, decoder_class = BaseDecoder):\n",
        "        super().__init__()\n",
        "        self.latent_size = latent_size\n",
        "        if encoder_class is not None: self.encoder = encoder_class(latent_size)\n",
        "        if decoder_class is not None: self.decoder = decoder_class(latent_size)\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> typing.Tuple[torch.Tensor, torch.Tensor]:\n",
        "        embedding = self.encoder(input)\n",
        "        recovered_input = self.decoder(embedding)\n",
        "        return recovered_input, embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_encoder(encoder: BaseEncoder):\n",
        "    dummy = torch.randn((10, 1, 32, 32))\n",
        "    assert encoder(dummy).shape == (10, encoder.latent_size)\n",
        "\n",
        "def test_decoder(decoder: BaseDecoder):\n",
        "    dummy = torch.randn((10, decoder.latent_size))\n",
        "    assert decoder(dummy).shape == (10, 1, 32, 32)\n",
        "\n",
        "def test_autoencoder(autoencoder: BaseAutoEncoder):\n",
        "    test_encoder(autoencoder.encoder)\n",
        "    test_decoder(autoencoder.decoder)\n",
        "    dummy = torch.randn((10, 1, 32, 32))\n",
        "    outputs, embeddings = autoencoder(dummy)\n",
        "    assert outputs.shape == dummy.shape\n",
        "    assert embeddings.shape == (10, autoencoder.latent_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def autoencoder_loss(input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.nn.functional.mse_loss(input, target, reduction = 'sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X56WacS2kMgj"
      },
      "outputs": [],
      "source": [
        "def lr_scheduler(epoch: int):\n",
        "    if epoch < 15: return 1\n",
        "    if epoch < 20: return 0.1\n",
        "    return 0.001\n",
        "\n",
        "def train(model: BaseAutoEncoder, name: str, learning_rate: float = 1e-3, n_epochs: int = 25) -> BaseAutoEncoder:\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_scheduler)\n",
        "    wandb.init(project = \"CV-HW-8\", name = name, anonymous = \"allow\")\n",
        "    wandb.watch(model, log = \"all\")\n",
        "    for epoch in tqdm.trange(n_epochs):\n",
        "        train_loss = 0\n",
        "        for (images, targets) in train_loader:\n",
        "            model.train() # Enter train mode\n",
        "            optimizer.zero_grad() # Zero gradients\n",
        "            output, _ = model(images.to(device)) # Get predictions\n",
        "            loss = autoencoder_loss(output, images.to(device)) # Calculate loss\n",
        "            loss.backward() # Calculate gradients\n",
        "            optimizer.step() # Update weights\n",
        "            wandb.log({ 'Train batch loss': loss.item() / images.shape[0] }) # Log metric\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        model.eval() # Enter eval mode\n",
        "        with torch.no_grad():\n",
        "            all_images = torch.empty((0, 1, 32, 32))\n",
        "            all_outputs = torch.empty((0, 1, 32, 32))\n",
        "            for (images, targets) in test_loader:\n",
        "                outputs = model(images.to(device))[0].detach().cpu()\n",
        "                all_images = torch.cat([ all_images, images ], dim = 0)\n",
        "                all_outputs = torch.cat([ all_outputs, outputs ], dim = 0)\n",
        "            test_loss = autoencoder_loss(all_outputs, all_images)\n",
        "\n",
        "        scheduler.step()\n",
        "        new_lr = optimizer.param_groups[0]['lr']\n",
        "        wandb.log({ 'Train loss': train_loss / len(train_ds), 'Test loss': test_loss / len(test_ds), 'Learning rate': new_lr })\n",
        "\n",
        "    wandb.finish()\n",
        "    return model.cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_predictions(model: BaseAutoEncoder) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    model = model.eval().to(device)\n",
        "    all_targets = torch.empty((0,))\n",
        "    all_images = torch.empty((0, 1, 32, 32))\n",
        "    all_outputs = torch.empty((0, 1, 32, 32))\n",
        "    all_embeddings = torch.empty((0, model.latent_size))\n",
        "    for (images, targets) in test_loader:\n",
        "        all_targets = torch.cat([ all_targets, targets ], dim = 0)\n",
        "        all_images = torch.cat([ all_images, images ], dim = 0)\n",
        "        with torch.no_grad():\n",
        "            outputs, embeddings = model(images.to(device))\n",
        "        all_outputs = torch.cat([ all_outputs, outputs.detach().cpu() ], dim = 0)\n",
        "        all_embeddings = torch.cat([ all_embeddings, embeddings.detach().cpu() ], dim = 0)\n",
        "    model = model.cpu()\n",
        "    return all_images, all_targets, all_embeddings, all_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_examples(model: BaseAutoEncoder):\n",
        "    plt.rcParams[\"figure.figsize\"] = (15, 5)\n",
        "    images, targets, embeddings, outputs = get_predictions(model)\n",
        "    ds = torchdata.TensorDataset(images, targets, embeddings, outputs)\n",
        "    loader = torchdata.DataLoader(ds, batch_size = 10, shuffle = True)\n",
        "    for i, (image, target, embedding, output) in enumerate(zip(*next(iter(loader)))):\n",
        "        plt.subplot(3, 10, i + 1)\n",
        "        display_image(image, target)\n",
        "        plt.subplot(3, 10, i + 11)\n",
        "        display_image(output, \"MSE {:.3f}\".format(autoencoder_loss(output, image)))\n",
        "        plt.subplot(3, 10, i + 21)\n",
        "        plt.axis('off')\n",
        "        plt.title('Difference')\n",
        "        diff = (denormalize(output) - denormalize(image) + 1) / 2\n",
        "        plt.imshow(diff.permute(1, 2, 0), cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_embeddings(model: BaseAutoEncoder, n_dimensions: int = 2):\n",
        "    images, targets, embeddings, outputs = get_predictions(model)\n",
        "    tsne = sklearn.manifold.TSNE(n_components = n_dimensions)\n",
        "    mapping = tsne.fit_transform(embeddings)\n",
        "    match n_dimensions:\n",
        "        case 2:\n",
        "            plt.rcParams[\"figure.figsize\"] = (15, 5)\n",
        "            seaborn.scatterplot(x = mapping[:, 0], y = mapping[:, 1], hue = targets, palette = seaborn.color_palette(\"hls\", 10))\n",
        "        case 3:\n",
        "            plt.rcParams[\"figure.figsize\"] = (7, 7)\n",
        "            plt.subplot(projection = '3d')\n",
        "            plt.scatter(mapping[:, 0], mapping[:, 1], mapping[:, 2], c = targets, cmap = 'tab10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_arithmetics(model: BaseAutoEncoder):\n",
        "    plt.rcParams[\"figure.figsize\"] = (15, 5)\n",
        "    images, targets, embeddings, outputs = get_predictions(model)\n",
        "    ds = torchdata.TensorDataset(images, targets, embeddings, outputs)\n",
        "    first_loader = torchdata.DataLoader(ds, batch_size = 10, shuffle = True)\n",
        "    second_loader = torchdata.DataLoader(ds, batch_size = 10, shuffle = True)\n",
        "    for i, (image1, target1, embedding1, output1, image2, target2, embedding2, output2) in enumerate(zip(*next(iter(first_loader)), *next(iter(second_loader)))):\n",
        "        output = model.decoder((embedding1 + embedding2).unsqueeze(0))[0]\n",
        "        plt.subplot(3, 10, i + 1)\n",
        "        display_image(image1, target1)\n",
        "        plt.subplot(3, 10, i + 11)\n",
        "        display_image(image2, target2)\n",
        "        plt.subplot(3, 10, i + 21)\n",
        "        display_image(output, \"{} + {}\".format(target1, target2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleEncoder(BaseEncoder):\n",
        "    def __init__(self, latent_size: int):\n",
        "        super().__init__(latent_size)\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 32, kernel_size = 3, stride = 2, padding = 1), torch.nn.ReLU(), # 32x32 -> 16x16\n",
        "            torch.nn.Conv2d(32, 64, kernel_size = 3, stride = 2, padding = 1), torch.nn.ReLU(), # 16x16 -> 8x8\n",
        "            torch.nn.Conv2d(64, 128, kernel_size = 3, stride = 2, padding = 1), torch.nn.ReLU(), # 8x8 -> 4x4\n",
        "            torch.nn.Flatten(), torch.nn.Linear(2048, latent_size)\n",
        "        )\n",
        "\n",
        "class SimpleDecoder(BaseDecoder):\n",
        "    def __init__(self, latent_size: int):\n",
        "        super().__init__(latent_size)\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(latent_size, 2048), torch.nn.Unflatten(1, (128, 4, 4)), torch.nn.ReLU(),\n",
        "            torch.nn.ConvTranspose2d(128, 64, kernel_size = 3, stride = 2, output_padding = 1, padding = 1), torch.nn.ReLU(), # 4x4 -> 8x8\n",
        "            torch.nn.ConvTranspose2d(64, 32, kernel_size = 3, stride = 2, output_padding = 1, padding = 1), torch.nn.ReLU(), # 8x8 -> 16x16\n",
        "            torch.nn.ConvTranspose2d(32, 1, kernel_size = 3, stride = 2, output_padding = 1, padding = 1) # 16x16 -> 32x32\n",
        "        )\n",
        "\n",
        "test_encoder(SimpleEncoder(128))\n",
        "test_decoder(SimpleDecoder(128))\n",
        "test_autoencoder(BaseAutoEncoder(128, SimpleEncoder, SimpleDecoder))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "simple_autoencoder = train(BaseAutoEncoder(128, SimpleEncoder, SimpleDecoder), name = 'SimpleAutoEncoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_examples(simple_autoencoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(simple_autoencoder, n_dimensions = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(simple_autoencoder, n_dimensions = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "test_arithmetics(simple_autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Better autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Попробуй собрать более сложную архитектуру."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BetterEncoder(BaseEncoder):\n",
        "    def __init__(self, latent_size: int):\n",
        "        super().__init__(latent_size)\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 16, kernel_size = 3, padding = 1), torch.nn.ReLU(), # 32x32\n",
        "            torch.nn.Conv2d(16, 32, kernel_size = 3, padding = 1), torch.nn.ReLU(), # 32x32\n",
        "            torch.nn.Conv2d(32, 64, kernel_size = 3, stride = 2, padding = 1), torch.nn.ReLU(), # 32x32 -> 16x16\n",
        "            torch.nn.Conv2d(64, 64, kernel_size = 3, padding = 1), torch.nn.ReLU(), # 16x16\n",
        "            torch.nn.Conv2d(64, 128, kernel_size = 3, stride = 2, padding = 1), torch.nn.ReLU(), # 16x16 -> 8x8\n",
        "            torch.nn.Conv2d(128, 128, kernel_size = 3, padding = 1), torch.nn.ReLU(), # 8x8\n",
        "            torch.nn.Conv2d(128, 256, kernel_size = 3, stride = 2, padding = 1), torch.nn.ReLU(), # 8x8 -> 4x4\n",
        "            torch.nn.Flatten(), torch.nn.Linear(256 * 4 * 4, latent_size)\n",
        "        )\n",
        "\n",
        "class BetterDecoder(BaseDecoder):\n",
        "    def __init__(self, latent_size: int):\n",
        "        super().__init__(latent_size)\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(latent_size, 256 * 4 * 4), torch.nn.Unflatten(1, (256, 4, 4)), torch.nn.ReLU(), # 4x4\n",
        "            torch.nn.ConvTranspose2d(256, 128, kernel_size = 3, stride = 2, output_padding = 1, padding = 1), torch.nn.ReLU(), # 4x4 -> 8x8\n",
        "            torch.nn.Conv2d(128, 128, kernel_size = 3, padding = 1), torch.nn.ReLU(), # 8x8\n",
        "            torch.nn.ConvTranspose2d(128, 64, kernel_size = 3, stride = 2, output_padding = 1, padding = 1), torch.nn.ReLU(), # 8x8 -> 16x16\n",
        "            torch.nn.Conv2d(64, 64, kernel_size = 3, padding = 1), torch.nn.ReLU(), # 8x8\n",
        "            torch.nn.ConvTranspose2d(64, 32, kernel_size = 3, stride = 2, output_padding = 1, padding = 1), torch.nn.ReLU(), # 16x16 -> 32x32\n",
        "            torch.nn.Conv2d(32, 16, kernel_size = 3, padding = 1), torch.nn.ReLU(), # 32x32\n",
        "            torch.nn.Conv2d(16, 1, kernel_size = 3, padding = 1) # 32x32\n",
        "        )\n",
        "\n",
        "test_encoder(BetterEncoder(128))\n",
        "test_decoder(BetterDecoder(128))\n",
        "test_autoencoder(BaseAutoEncoder(128, BetterEncoder, BetterDecoder))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "better_autoencoder = train(BaseAutoEncoder(128, BetterEncoder, BetterDecoder), name = 'BetterAutoEncoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_examples(better_autoencoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(better_autoencoder, n_dimensions = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(better_autoencoder, n_dimensions = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "test_arithmetics(better_autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Добавим слои нормализации. Очевидно, BatchNorm для решения поставленной задачи подходит не очень хорошо. Будем использовать InstanceNorm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NormalizedEncoder(BaseEncoder):\n",
        "    def __init__(self, latent_size: int):\n",
        "        super().__init__(latent_size)\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 16, kernel_size = 3, padding = 1), torch.nn.InstanceNorm2d(16), torch.nn.ReLU(), # 32x32\n",
        "            torch.nn.Conv2d(16, 32, kernel_size = 3, padding = 1), torch.nn.InstanceNorm2d(32), torch.nn.ReLU(), # 32x32\n",
        "            torch.nn.Conv2d(32, 64, kernel_size = 3, stride = 2, padding = 1), torch.nn.InstanceNorm2d(64), torch.nn.ReLU(), # 32x32 -> 16x16\n",
        "            torch.nn.Conv2d(64, 64, kernel_size = 3, padding = 1), torch.nn.InstanceNorm2d(64), torch.nn.ReLU(), # 16x16\n",
        "            torch.nn.Conv2d(64, 128, kernel_size = 3, stride = 2, padding = 1), torch.nn.InstanceNorm2d(128), torch.nn.ReLU(), # 16x16 -> 8x8\n",
        "            torch.nn.Conv2d(128, 128, kernel_size = 3, padding = 1), torch.nn.InstanceNorm2d(128), torch.nn.ReLU(), # 8x8\n",
        "            torch.nn.Conv2d(128, 256, kernel_size = 3, stride = 2, padding = 1), torch.nn.InstanceNorm2d(256), torch.nn.ReLU(), # 8x8 -> 4x4\n",
        "            torch.nn.Flatten(), torch.nn.Linear(256 * 4 * 4, latent_size)\n",
        "        )\n",
        "\n",
        "class NormalizedDecoder(BaseDecoder):\n",
        "    def __init__(self, latent_size: int):\n",
        "        super().__init__(latent_size)\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.InstanceNorm1d(256), torch.nn.Linear(latent_size, 256 * 4 * 4),\n",
        "            torch.nn.InstanceNorm1d(256 * 4 * 4), torch.nn.Unflatten(1, (256, 4, 4)), torch.nn.ReLU(), # 4x4\n",
        "            torch.nn.ConvTranspose2d(256, 128, kernel_size = 3, stride = 2, output_padding = 1, padding = 1), torch.nn.InstanceNorm2d(128), torch.nn.ReLU(), # 4x4 -> 8x8\n",
        "            torch.nn.Conv2d(128, 128, kernel_size = 3, padding = 1), torch.nn.InstanceNorm2d(128), torch.nn.ReLU(), # 8x8\n",
        "            torch.nn.ConvTranspose2d(128, 64, kernel_size = 3, stride = 2, output_padding = 1, padding = 1), torch.nn.InstanceNorm2d(64), torch.nn.ReLU(), # 8x8 -> 16x16\n",
        "            torch.nn.Conv2d(64, 64, kernel_size = 3, padding = 1), torch.nn.InstanceNorm2d(64), torch.nn.ReLU(), # 8x8\n",
        "            torch.nn.ConvTranspose2d(64, 32, kernel_size = 3, stride = 2, output_padding = 1, padding = 1), torch.nn.InstanceNorm2d(32), torch.nn.ReLU(), # 16x16 -> 32x32\n",
        "            torch.nn.Conv2d(32, 16, kernel_size = 3, padding = 1), torch.nn.InstanceNorm2d(16), torch.nn.ReLU(), # 32x32\n",
        "            torch.nn.Conv2d(16, 1, kernel_size = 3, padding = 1) # 32x32\n",
        "        )\n",
        "\n",
        "test_encoder(NormalizedEncoder(128))\n",
        "test_decoder(NormalizedDecoder(128))\n",
        "test_autoencoder(BaseAutoEncoder(128, NormalizedEncoder, NormalizedDecoder))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "normalized_autoencoder = train(BaseAutoEncoder(128, NormalizedEncoder, NormalizedDecoder), name = 'NormalizedAutoEncoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_examples(normalized_autoencoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(normalized_autoencoder, n_dimensions = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(normalized_autoencoder, n_dimensions = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "test_arithmetics(normalized_autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Попробуем добавить Dropout: потребуем от модели верно восстанавливать картинку даже по части вектора-эмбеддинга."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DropoutEncoder(BaseEncoder):\n",
        "    def __init__(self, latent_size: int):\n",
        "        super().__init__(latent_size)\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 16, kernel_size = 3, padding = 1), torch.nn.InstanceNorm2d(16), torch.nn.ReLU(), # 32x32\n",
        "            torch.nn.Conv2d(16, 32, kernel_size = 3, padding = 1), torch.nn.InstanceNorm2d(32), torch.nn.ReLU(), # 32x32\n",
        "            torch.nn.Conv2d(32, 64, kernel_size = 3, stride = 2, padding = 1), torch.nn.InstanceNorm2d(64), torch.nn.ReLU(), # 32x32 -> 16x16\n",
        "            torch.nn.Conv2d(64, 64, kernel_size = 3, padding = 1), torch.nn.InstanceNorm2d(64), torch.nn.ReLU(), # 16x16\n",
        "            torch.nn.Conv2d(64, 128, kernel_size = 3, stride = 2, padding = 1), torch.nn.InstanceNorm2d(128), torch.nn.ReLU(), # 16x16 -> 8x8\n",
        "            torch.nn.Conv2d(128, 128, kernel_size = 3, padding = 1), torch.nn.InstanceNorm2d(128), torch.nn.ReLU(), # 8x8\n",
        "            torch.nn.Conv2d(128, 256, kernel_size = 3, stride = 2, padding = 1), torch.nn.InstanceNorm2d(256), torch.nn.ReLU(), # 8x8 -> 4x4\n",
        "            torch.nn.Flatten(), torch.nn.Dropout(0.05), torch.nn.Linear(256 * 4 * 4, latent_size)\n",
        "        )\n",
        "\n",
        "class DropoutDecoder(BaseDecoder):\n",
        "    def __init__(self, latent_size: int):\n",
        "        super().__init__(latent_size)\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.InstanceNorm1d(256), torch.nn.Dropout(0.05), torch.nn.Linear(latent_size, 256 * 4 * 4),\n",
        "            torch.nn.InstanceNorm1d(256 * 4 * 4), torch.nn.Unflatten(1, (256, 4, 4)), torch.nn.ReLU(), # 4x4\n",
        "            torch.nn.ConvTranspose2d(256, 128, kernel_size = 3, stride = 2, output_padding = 1, padding = 1), torch.nn.InstanceNorm2d(128), torch.nn.ReLU(), # 4x4 -> 8x8\n",
        "            torch.nn.Conv2d(128, 128, kernel_size = 3, padding = 1), torch.nn.InstanceNorm2d(128), torch.nn.ReLU(), # 8x8\n",
        "            torch.nn.ConvTranspose2d(128, 64, kernel_size = 3, stride = 2, output_padding = 1, padding = 1), torch.nn.InstanceNorm2d(64), torch.nn.ReLU(), # 8x8 -> 16x16\n",
        "            torch.nn.Conv2d(64, 64, kernel_size = 3, padding = 1), torch.nn.InstanceNorm2d(64), torch.nn.ReLU(), # 8x8\n",
        "            torch.nn.ConvTranspose2d(64, 32, kernel_size = 3, stride = 2, output_padding = 1, padding = 1), torch.nn.InstanceNorm2d(32), torch.nn.ReLU(), # 16x16 -> 32x32\n",
        "            torch.nn.Conv2d(32, 16, kernel_size = 3, padding = 1), torch.nn.InstanceNorm2d(16), torch.nn.ReLU(), # 32x32\n",
        "            torch.nn.Conv2d(16, 1, kernel_size = 3, padding = 1) # 32x32\n",
        "        )\n",
        "\n",
        "test_encoder(DropoutEncoder(128))\n",
        "test_decoder(DropoutDecoder(128))\n",
        "test_autoencoder(BaseAutoEncoder(128, DropoutEncoder, DropoutDecoder))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "dropout_autoencoder = train(BaseAutoEncoder(128, DropoutEncoder, DropoutDecoder), name = 'DropoutAutoEncoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_examples(dropout_autoencoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(dropout_autoencoder, n_dimensions = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(dropout_autoencoder, n_dimensions = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "test_arithmetics(dropout_autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Difference embedding size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Попробуем изменить размерность эмбеддингов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "best_autoencoder_32 = train(BaseAutoEncoder(32, NormalizedEncoder, NormalizedDecoder), name = 'Autoencoder - 32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_examples(best_autoencoder_32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(best_autoencoder_32, n_dimensions = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(best_autoencoder_32, n_dimensions = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "test_arithmetics(best_autoencoder_32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "best_autoencoder_64 = train(BaseAutoEncoder(64, NormalizedEncoder, NormalizedDecoder), name = 'Autoencoder - 64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_examples(best_autoencoder_64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(best_autoencoder_64, n_dimensions = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(best_autoencoder_64, n_dimensions = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "test_arithmetics(best_autoencoder_64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "best_autoencoder_256 = train(BaseAutoEncoder(256, NormalizedEncoder, NormalizedDecoder), name = 'Autoencoder - 256')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_examples(best_autoencoder_256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(best_autoencoder_256, n_dimensions = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(best_autoencoder_256, n_dimensions = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "test_arithmetics(best_autoencoder_256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "best_autoencoder_512 = train(BaseAutoEncoder(512, NormalizedEncoder, NormalizedDecoder), name = 'Autoencoder - 512')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_examples(best_autoencoder_512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(best_autoencoder_512, n_dimensions = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "show_embeddings(best_autoencoder_512, n_dimensions = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix_random()\n",
        "test_arithmetics(best_autoencoder_512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_dkuRBHCmCR"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
