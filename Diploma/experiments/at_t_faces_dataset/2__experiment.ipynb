{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.v2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = torchvision.transforms.v2.Compose([\n",
    "    # torchvision.transforms.v2.Resize((96, 96), antialias = True),\n",
    "    torchvision.transforms.v2.Grayscale(),\n",
    "    torchvision.transforms.v2.ToImage(),\n",
    "    torchvision.transforms.v2.ToDtype(torch.float32, scale = True),\n",
    "    # torchvision.transforms.v2.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "train = torchvision.datasets.ImageFolder(\"faces/training\", transform = transform)\n",
    "test = torchvision.datasets.ImageFolder(\"faces/testing\", transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 370/370 [00:00<00:00, 3261.52it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 2934.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((370, 112, 92), (30, 112, 92))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "import numpy\n",
    "\n",
    "train_images = numpy.array([ numpy.array(item[0]).squeeze() for item in tqdm.tqdm(train) ])\n",
    "train_labels = numpy.array([ item[1] for item in train ])\n",
    "\n",
    "test_images = numpy.array([ numpy.array(item[0]).squeeze() for item in tqdm.tqdm(test) ])\n",
    "test_labels = numpy.array([ item[1] for item in test ])\n",
    "\n",
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "import typing\n",
    "\n",
    "import tqdm\n",
    "import numpy\n",
    "import joblib\n",
    "import gtda.images\n",
    "\n",
    "import cvtda.utils\n",
    "import cvtda.topology\n",
    "\n",
    "def make_diagrams(\n",
    "    binarizer,\n",
    "    filtration,\n",
    "    n_jobs: int = 1\n",
    ") -> typing.Tuple[numpy.ndarray, numpy.ndarray]:\n",
    "    dir = f\"1/{str(filtration or 'None')}\"\n",
    "    if os.path.exists(f\"{dir}/test_features.npy\"):\n",
    "        return 1, 2\n",
    "    os.makedirs(dir, exist_ok = True)\n",
    "    \n",
    "    with joblib.parallel_backend(\"loky\", inner_max_num_threads = n_jobs):\n",
    "        train = train_images.copy()\n",
    "        test = test_images.copy()\n",
    "\n",
    "        if binarizer is not None:\n",
    "            train = binarizer.fit_transform(train)\n",
    "            test = binarizer.transform(test)\n",
    "            \n",
    "        if filtration is not None:\n",
    "            train = filtration.fit_transform(train)\n",
    "            test = filtration.transform(test)\n",
    "        \n",
    "        filtrations_to_diagrams = cvtda.topology.FiltrationsToDiagrams(verbose = False, n_jobs = n_jobs)\n",
    "        train = filtrations_to_diagrams.fit_transform(train)\n",
    "        test = filtrations_to_diagrams.transform(test)\n",
    "\n",
    "    numpy.save(f\"{dir}/train_diagrams.npy\", train)\n",
    "    numpy.save(f\"{dir}/test_diagrams.npy\", test)\n",
    "    \n",
    "    if len(train[0]) < 96:\n",
    "        n_bins = 32\n",
    "    elif len(train[0]) < 192:\n",
    "        n_bins = 64\n",
    "    else:\n",
    "        n_bins = 128\n",
    "\n",
    "    with joblib.parallel_backend(\"loky\", inner_max_num_threads = n_jobs):\n",
    "        digrams_to_features = cvtda.topology.DiagramsToFeatures(batch_size = 500, n_bins = n_bins, verbose = False, n_jobs = n_jobs)\n",
    "        train = digrams_to_features.fit_transform(train)\n",
    "        test = digrams_to_features.transform(test)\n",
    "\n",
    "    numpy.save(f\"{dir}/train_features.npy\", train)\n",
    "    numpy.save(f\"{dir}/test_features.npy\", test)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def process(binarizer_threshold: float) -> typing.Tuple[numpy.ndarray, numpy.ndarray]:\n",
    "    centers = [ 5, 12, 18, 25 ]\n",
    "    greyscale_to_filtrations = cvtda.topology.GreyscaleToFiltrations(\n",
    "        n_jobs = 2,\n",
    "        radial_filtration_centers = list(itertools.product(centers, centers))\n",
    "    )\n",
    "    diagrams = joblib.Parallel(return_as = 'generator', n_jobs = 8)(\n",
    "        joblib.delayed(make_diagrams)(\n",
    "            binarizer = gtda.images.Binarizer(threshold = binarizer_threshold, n_jobs = 1),\n",
    "            filtration = filtration,\n",
    "            n_jobs = 2\n",
    "        )\n",
    "        for filtration in greyscale_to_filtrations.filtrations_\n",
    "    )\n",
    "    for train, test in tqdm.tqdm(diagrams, total = len(greyscale_to_filtrations.filtrations_)):\n",
    "        pass\n",
    "\n",
    "    make_diagrams(\n",
    "        binarizer = None,\n",
    "        filtration = None,\n",
    "        n_jobs = -1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 140.10it/s]\n"
     ]
    }
   ],
   "source": [
    "process(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:03<00:00,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 torch.Size([370, 102, 2]) torch.Size([370, 102])\n",
      "100 torch.Size([30, 66, 2]) torch.Size([30, 66])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import joblib\n",
    "import torch\n",
    "import gtda.diagrams\n",
    "import torchph.nn.slayer\n",
    "\n",
    "def process():\n",
    "    def transform(diagram, dim):\n",
    "        dim_filter = (diagram[:, 2] == dim)\n",
    "        non_degenerate_filter = (diagram[:, 0] != diagram[:, 1])\n",
    "        rotation = torchph.nn.slayer.UpperDiagonalThresholdedLogTransform(0.05)\n",
    "        return rotation(diagram[dim_filter & non_degenerate_filter][:, 0:2])\n",
    "\n",
    "    train_data = [ ]\n",
    "    test_data = [ ]\n",
    "    for filtration in tqdm.tqdm(os.listdir(f\"1\")):\n",
    "        for dim in [ 0, 1 ]:\n",
    "                dir = f\"1/{filtration}\"\n",
    "                train_diagrams = numpy.load(f\"{dir}/train_diagrams.npy\")\n",
    "                test_diagrams = numpy.load(f\"{dir}/test_diagrams.npy\")\n",
    "\n",
    "                scaler = gtda.diagrams.Scaler()\n",
    "                train_diagrams = scaler.fit_transform(train_diagrams)\n",
    "                test_diagrams = scaler.transform(test_diagrams)\n",
    "                \n",
    "                # filtering = gtda.diagrams.Filtering()\n",
    "                # train_diagrams = filtering.fit_transform(train_diagrams)\n",
    "                # test_diagrams = filtering.transform(test_diagrams)\n",
    "                \n",
    "                train_diagrams = torch.tensor(train_diagrams, dtype = torch.float32)\n",
    "                test_diagrams = torch.tensor(test_diagrams, dtype = torch.float32)\n",
    "\n",
    "                diagrams_train = joblib.Parallel(n_jobs = 1)(joblib.delayed(transform)(diagram, dim) for diagram in train_diagrams)\n",
    "                diagrams, non_dummy_points, _, _ = torchph.nn.slayer.prepare_batch(diagrams_train)\n",
    "                train_data.append(diagrams)\n",
    "                train_data.append(non_dummy_points)\n",
    "                \n",
    "                diagrams_test = joblib.Parallel(n_jobs = 1)(joblib.delayed(transform)(diagram, dim) for diagram in test_diagrams)\n",
    "                diagrams, non_dummy_points, _, _ = torchph.nn.slayer.prepare_batch(diagrams_test)\n",
    "                test_data.append(diagrams)\n",
    "                test_data.append(non_dummy_points)\n",
    "    return train_data, test_data\n",
    "\n",
    "train_diagrams, test_diagrams = process()\n",
    " \n",
    "print(len(train_diagrams), train_diagrams[0].shape, train_diagrams[1].shape)\n",
    "print(len(test_diagrams), test_diagrams[0].shape, test_diagrams[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_labels, dtype = torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pytorch_metric_learning.samplers\n",
    "\n",
    "def show_class_distribtion(labels, classes):\n",
    "    plt.figure(figsize = (10, 3))\n",
    "    x, counts = labels.unique(return_counts = True)\n",
    "    plt.bar(numpy.array(classes)[x], height = counts)\n",
    "\n",
    "N_EPOCHS = 25\n",
    "LENGTH_BEFORE_NEW_ITER = 1280\n",
    "train_mpc_sampler = pytorch_metric_learning.samplers.MPerClassSampler(labels = train_labels, m = 4, length_before_new_iter = LENGTH_BEFORE_NEW_ITER)\n",
    "train_set = torch.utils.data.TensorDataset(train_labels, *train_diagrams)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 64, sampler = train_mpc_sampler)\n",
    "show_class_distribtion(next(iter(train_loader))[0], train.classes)\n",
    "\n",
    "\n",
    "test_set = torch.utils.data.TensorDataset(test_labels, *test_diagrams)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = len(test_set))\n",
    "show_class_distribtion(next(iter(test_loader))[0], test.classes)\n",
    "\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_elements = 128\n",
    "        self.n_channels = 50\n",
    "        self.slayers = [\n",
    "            torchph.nn.slayer.SLayerExponential(self.n_elements).to(device)\n",
    "            for _ in range(self.n_channels)\n",
    "        ]\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.2), torch.nn.Linear(self.n_channels * self.n_elements, 1024), torch.nn.BatchNorm1d(1024), torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1), torch.nn.Linear(1024, 512), torch.nn.BatchNorm1d(512), torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 256)\n",
    "        ).to(device)\n",
    "    \n",
    "    def forward(self, args):\n",
    "        features = [ ]\n",
    "        for i in range(0, len(args), 2):\n",
    "            slayer_args = (args[i].to(device), args[i + 1].to(device), args[i].shape[1], len(args[i]))\n",
    "            features.append(self.slayers[i // 2](slayer_args))\n",
    "        return self.classifier(torch.cat(features, dim = 1))\n",
    "    \n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN = 0.5 # Standard value in PyTorch TripletMarginLoss, should be good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256])\n",
      "3 torch.Size([11379]) torch.Size([11379]) torch.Size([11379])\n"
     ]
    }
   ],
   "source": [
    "import pytorch_metric_learning.miners\n",
    "\n",
    "train_miner = pytorch_metric_learning.miners.TripletMarginMiner(margin = MARGIN, type_of_triplets = \"all\")\n",
    "\n",
    "labels, *images = next(iter(train_loader))\n",
    "embeddings = model(images)\n",
    "print(embeddings.shape)\n",
    "\n",
    "indices = train_miner(embeddings, labels)\n",
    "print(len(indices), indices[0].shape, indices[1].shape, indices[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3968, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import pytorch_metric_learning.losses\n",
    "\n",
    "criterion = pytorch_metric_learning.losses.TripletMarginLoss(margin = MARGIN)\n",
    "loss = criterion(embeddings, labels, indices)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AMI': 0.5659344147055957,\n",
       " 'NMI': 0.5957668744210505,\n",
       " 'mean_average_precision': 0.8858484752516401,\n",
       " 'mean_average_precision_at_r': 0.7801293356848913,\n",
       " 'mean_reciprocal_rank': 1.0,\n",
       " 'precision_at_1': 1.0,\n",
       " 'r_precision': 0.7814814814814814}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_metric_learning.utils.accuracy_calculator\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "metrics = pytorch_metric_learning.utils.accuracy_calculator.AccuracyCalculator()\n",
    "def validate(model: torch.nn.Module) -> dict:\n",
    "    with torch.no_grad():\n",
    "        targets, *images = next(iter(test_loader))\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        embeddings = model(images)\n",
    "        return metrics.get_accuracy(embeddings, targets.to(device))\n",
    "\n",
    "validate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 19/19 [00:04<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.36261, Test MAP: 0.92042, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 19/19 [00:04<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.34608, Test MAP: 0.98909, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 19/19 [00:04<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.32768, Test MAP: 0.99401, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 19/19 [00:04<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.32242, Test MAP: 0.99553, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 19/19 [00:04<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.31428, Test MAP: 0.99745, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 19/19 [00:04<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.30399, Test MAP: 0.99854, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 19/19 [00:04<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.29484, Test MAP: 0.98765, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 19/19 [00:04<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.29196, Test MAP: 0.99870, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 19/19 [00:04<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.29189, Test MAP: 0.99348, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 19/19 [00:04<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.28114, Test MAP: 0.99745, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 19/19 [00:04<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.26687, Test MAP: 0.99752, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 19/19 [00:04<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.26589, Test MAP: 0.99665, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 19/19 [00:04<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.26808, Test MAP: 0.99800, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 19/19 [00:04<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.24958, Test MAP: 0.99729, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 19/19 [00:04<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.25463, Test MAP: 0.99673, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 19/19 [00:04<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.25960, Test MAP: 0.99445, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 19/19 [00:04<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.24722, Test MAP: 0.99711, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 19/19 [00:04<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.24274, Test MAP: 0.99774, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 19/19 [00:04<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.23354, Test MAP: 0.99737, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 19/19 [00:04<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.22730, Test MAP: 0.99733, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 19/19 [00:04<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.23612, Test MAP: 0.99636, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 19/19 [00:04<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.22848, Test MAP: 0.99460, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 19/19 [00:04<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.20997, Test MAP: 0.99404, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 19/19 [00:04<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.20419, Test MAP: 0.99854, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 19/19 [00:04<00:00,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.20675, Test MAP: 0.99476, Test AMI: 1.00000, Test NMI: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(model: torch.nn.Module, n_epochs: int) -> None:\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = 7e-5)\n",
    "    for epoch in range(n_epochs):\n",
    "        sum_loss = 0\n",
    "        for (targets, *images) in tqdm.tqdm(train_loader, desc = 'Epoch {}'.format(epoch + 1)):\n",
    "            if len(targets) == 0:\n",
    "                continue\n",
    "            model.train() # Enter train mode\n",
    "            optimizer.zero_grad() # Zero gradients\n",
    "            embeddings = model(images)\n",
    "            indices = train_miner(embeddings, targets)\n",
    "            loss = criterion(embeddings, targets.to(device), indices)\n",
    "            loss.backward() # Calculate gradients\n",
    "            optimizer.step() # Update weights\n",
    "            sum_loss += loss.item()\n",
    "        train_loss = sum_loss / len(train_loader)\n",
    "        metrics = validate(model)\n",
    "        mean_avg_pr = metrics['mean_average_precision']\n",
    "        AMI = metrics['AMI']\n",
    "        NMI = metrics['NMI']\n",
    "        print(f\"Train loss: {train_loss:.5f}, Test MAP: {mean_avg_pr:.5f}, Test AMI: {AMI:.5f}, Test NMI: {NMI:.5f}\")\n",
    "\n",
    "train(model, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AMI': 1.0,\n",
       " 'NMI': 1.0,\n",
       " 'mean_average_precision': 0.9947638780972115,\n",
       " 'mean_average_precision_at_r': 0.9814814814814815,\n",
       " 'mean_reciprocal_rank': 1.0,\n",
       " 'precision_at_1': 1.0,\n",
       " 'r_precision': 0.9814814814814815}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "validate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_embeddings = model(next(iter(test_loader))[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>4.96</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0.98</td>\n",
       "      <td>4.53</td>\n",
       "      <td>4.24</td>\n",
       "      <td>1.62</td>\n",
       "      <td>5.31</td>\n",
       "      <td>4.96</td>\n",
       "      <td>8.02</td>\n",
       "      <td>9.15</td>\n",
       "      <td>9.60</td>\n",
       "      <td>7.06</td>\n",
       "      <td>12.70</td>\n",
       "      <td>7.37</td>\n",
       "      <td>10.37</td>\n",
       "      <td>11.17</td>\n",
       "      <td>12.45</td>\n",
       "      <td>12.46</td>\n",
       "      <td>15.78</td>\n",
       "      <td>9.96</td>\n",
       "      <td>12.82</td>\n",
       "      <td>13.48</td>\n",
       "      <td>12.54</td>\n",
       "      <td>11.54</td>\n",
       "      <td>13.96</td>\n",
       "      <td>15.19</td>\n",
       "      <td>17.40</td>\n",
       "      <td>16.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.53</td>\n",
       "      <td>4.94</td>\n",
       "      <td>5.30</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.22</td>\n",
       "      <td>4.41</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.36</td>\n",
       "      <td>7.66</td>\n",
       "      <td>7.98</td>\n",
       "      <td>8.48</td>\n",
       "      <td>7.39</td>\n",
       "      <td>11.09</td>\n",
       "      <td>6.73</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.44</td>\n",
       "      <td>10.75</td>\n",
       "      <td>11.10</td>\n",
       "      <td>16.96</td>\n",
       "      <td>11.07</td>\n",
       "      <td>14.50</td>\n",
       "      <td>14.90</td>\n",
       "      <td>13.19</td>\n",
       "      <td>12.41</td>\n",
       "      <td>15.57</td>\n",
       "      <td>16.54</td>\n",
       "      <td>18.48</td>\n",
       "      <td>17.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.67</td>\n",
       "      <td>4.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.71</td>\n",
       "      <td>4.51</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.33</td>\n",
       "      <td>4.66</td>\n",
       "      <td>4.75</td>\n",
       "      <td>7.82</td>\n",
       "      <td>9.06</td>\n",
       "      <td>9.25</td>\n",
       "      <td>6.90</td>\n",
       "      <td>12.45</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10.06</td>\n",
       "      <td>10.84</td>\n",
       "      <td>12.06</td>\n",
       "      <td>12.05</td>\n",
       "      <td>14.77</td>\n",
       "      <td>9.00</td>\n",
       "      <td>12.02</td>\n",
       "      <td>12.51</td>\n",
       "      <td>11.63</td>\n",
       "      <td>10.59</td>\n",
       "      <td>13.05</td>\n",
       "      <td>14.18</td>\n",
       "      <td>16.35</td>\n",
       "      <td>15.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.36</td>\n",
       "      <td>4.94</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.82</td>\n",
       "      <td>4.56</td>\n",
       "      <td>4.02</td>\n",
       "      <td>1.65</td>\n",
       "      <td>4.61</td>\n",
       "      <td>5.22</td>\n",
       "      <td>9.27</td>\n",
       "      <td>10.35</td>\n",
       "      <td>10.66</td>\n",
       "      <td>8.26</td>\n",
       "      <td>13.65</td>\n",
       "      <td>8.63</td>\n",
       "      <td>11.46</td>\n",
       "      <td>12.12</td>\n",
       "      <td>13.40</td>\n",
       "      <td>13.27</td>\n",
       "      <td>15.07</td>\n",
       "      <td>9.75</td>\n",
       "      <td>12.27</td>\n",
       "      <td>12.91</td>\n",
       "      <td>12.49</td>\n",
       "      <td>11.42</td>\n",
       "      <td>13.41</td>\n",
       "      <td>14.39</td>\n",
       "      <td>16.62</td>\n",
       "      <td>15.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98</td>\n",
       "      <td>5.30</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.03</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.81</td>\n",
       "      <td>5.80</td>\n",
       "      <td>5.41</td>\n",
       "      <td>7.77</td>\n",
       "      <td>9.06</td>\n",
       "      <td>9.34</td>\n",
       "      <td>6.87</td>\n",
       "      <td>12.58</td>\n",
       "      <td>7.15</td>\n",
       "      <td>10.13</td>\n",
       "      <td>11.04</td>\n",
       "      <td>12.20</td>\n",
       "      <td>12.32</td>\n",
       "      <td>15.47</td>\n",
       "      <td>9.60</td>\n",
       "      <td>12.59</td>\n",
       "      <td>13.15</td>\n",
       "      <td>12.19</td>\n",
       "      <td>11.14</td>\n",
       "      <td>13.62</td>\n",
       "      <td>14.90</td>\n",
       "      <td>17.07</td>\n",
       "      <td>16.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.53</td>\n",
       "      <td>1.90</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.56</td>\n",
       "      <td>5.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>2.15</td>\n",
       "      <td>8.21</td>\n",
       "      <td>8.57</td>\n",
       "      <td>9.19</td>\n",
       "      <td>7.81</td>\n",
       "      <td>11.74</td>\n",
       "      <td>7.37</td>\n",
       "      <td>9.81</td>\n",
       "      <td>10.25</td>\n",
       "      <td>11.56</td>\n",
       "      <td>11.89</td>\n",
       "      <td>17.32</td>\n",
       "      <td>11.51</td>\n",
       "      <td>14.71</td>\n",
       "      <td>15.25</td>\n",
       "      <td>13.83</td>\n",
       "      <td>12.94</td>\n",
       "      <td>15.86</td>\n",
       "      <td>16.84</td>\n",
       "      <td>18.82</td>\n",
       "      <td>17.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.24</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.80</td>\n",
       "      <td>4.02</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.76</td>\n",
       "      <td>7.93</td>\n",
       "      <td>8.38</td>\n",
       "      <td>8.95</td>\n",
       "      <td>7.37</td>\n",
       "      <td>11.66</td>\n",
       "      <td>7.05</td>\n",
       "      <td>9.58</td>\n",
       "      <td>10.01</td>\n",
       "      <td>11.40</td>\n",
       "      <td>11.47</td>\n",
       "      <td>16.38</td>\n",
       "      <td>10.64</td>\n",
       "      <td>13.81</td>\n",
       "      <td>14.28</td>\n",
       "      <td>12.88</td>\n",
       "      <td>12.06</td>\n",
       "      <td>14.96</td>\n",
       "      <td>15.90</td>\n",
       "      <td>17.95</td>\n",
       "      <td>16.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.62</td>\n",
       "      <td>4.41</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.81</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.78</td>\n",
       "      <td>8.52</td>\n",
       "      <td>9.67</td>\n",
       "      <td>9.90</td>\n",
       "      <td>7.73</td>\n",
       "      <td>13.04</td>\n",
       "      <td>7.86</td>\n",
       "      <td>10.69</td>\n",
       "      <td>11.48</td>\n",
       "      <td>12.64</td>\n",
       "      <td>12.84</td>\n",
       "      <td>15.63</td>\n",
       "      <td>9.99</td>\n",
       "      <td>12.88</td>\n",
       "      <td>13.38</td>\n",
       "      <td>12.66</td>\n",
       "      <td>11.57</td>\n",
       "      <td>13.92</td>\n",
       "      <td>15.00</td>\n",
       "      <td>17.16</td>\n",
       "      <td>16.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.31</td>\n",
       "      <td>1.95</td>\n",
       "      <td>4.66</td>\n",
       "      <td>4.61</td>\n",
       "      <td>5.80</td>\n",
       "      <td>2.74</td>\n",
       "      <td>1.72</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.15</td>\n",
       "      <td>8.44</td>\n",
       "      <td>8.70</td>\n",
       "      <td>9.30</td>\n",
       "      <td>7.87</td>\n",
       "      <td>11.70</td>\n",
       "      <td>7.60</td>\n",
       "      <td>9.88</td>\n",
       "      <td>10.10</td>\n",
       "      <td>11.58</td>\n",
       "      <td>11.45</td>\n",
       "      <td>16.39</td>\n",
       "      <td>10.82</td>\n",
       "      <td>13.92</td>\n",
       "      <td>14.41</td>\n",
       "      <td>12.99</td>\n",
       "      <td>12.27</td>\n",
       "      <td>15.13</td>\n",
       "      <td>15.89</td>\n",
       "      <td>17.94</td>\n",
       "      <td>16.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.96</td>\n",
       "      <td>1.36</td>\n",
       "      <td>4.75</td>\n",
       "      <td>5.22</td>\n",
       "      <td>5.41</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1.76</td>\n",
       "      <td>4.78</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.16</td>\n",
       "      <td>7.27</td>\n",
       "      <td>8.11</td>\n",
       "      <td>6.74</td>\n",
       "      <td>10.55</td>\n",
       "      <td>6.23</td>\n",
       "      <td>8.64</td>\n",
       "      <td>8.92</td>\n",
       "      <td>10.46</td>\n",
       "      <td>10.49</td>\n",
       "      <td>17.07</td>\n",
       "      <td>11.07</td>\n",
       "      <td>14.45</td>\n",
       "      <td>14.98</td>\n",
       "      <td>13.09</td>\n",
       "      <td>12.43</td>\n",
       "      <td>15.67</td>\n",
       "      <td>16.65</td>\n",
       "      <td>18.67</td>\n",
       "      <td>17.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.02</td>\n",
       "      <td>7.66</td>\n",
       "      <td>7.82</td>\n",
       "      <td>9.27</td>\n",
       "      <td>7.77</td>\n",
       "      <td>8.21</td>\n",
       "      <td>7.93</td>\n",
       "      <td>8.52</td>\n",
       "      <td>8.44</td>\n",
       "      <td>7.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.33</td>\n",
       "      <td>2.26</td>\n",
       "      <td>5.57</td>\n",
       "      <td>1.54</td>\n",
       "      <td>3.02</td>\n",
       "      <td>4.06</td>\n",
       "      <td>5.39</td>\n",
       "      <td>5.28</td>\n",
       "      <td>15.82</td>\n",
       "      <td>9.00</td>\n",
       "      <td>13.62</td>\n",
       "      <td>13.82</td>\n",
       "      <td>10.16</td>\n",
       "      <td>9.89</td>\n",
       "      <td>14.44</td>\n",
       "      <td>15.75</td>\n",
       "      <td>17.31</td>\n",
       "      <td>16.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.15</td>\n",
       "      <td>7.98</td>\n",
       "      <td>9.06</td>\n",
       "      <td>10.35</td>\n",
       "      <td>9.06</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8.38</td>\n",
       "      <td>9.67</td>\n",
       "      <td>8.70</td>\n",
       "      <td>7.27</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.07</td>\n",
       "      <td>3.45</td>\n",
       "      <td>4.55</td>\n",
       "      <td>2.61</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.11</td>\n",
       "      <td>5.24</td>\n",
       "      <td>4.51</td>\n",
       "      <td>17.12</td>\n",
       "      <td>10.61</td>\n",
       "      <td>14.83</td>\n",
       "      <td>15.21</td>\n",
       "      <td>11.52</td>\n",
       "      <td>11.47</td>\n",
       "      <td>15.87</td>\n",
       "      <td>17.05</td>\n",
       "      <td>18.68</td>\n",
       "      <td>17.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.60</td>\n",
       "      <td>8.48</td>\n",
       "      <td>9.25</td>\n",
       "      <td>10.66</td>\n",
       "      <td>9.34</td>\n",
       "      <td>9.19</td>\n",
       "      <td>8.95</td>\n",
       "      <td>9.90</td>\n",
       "      <td>9.30</td>\n",
       "      <td>8.11</td>\n",
       "      <td>2.33</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.35</td>\n",
       "      <td>4.01</td>\n",
       "      <td>2.83</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.34</td>\n",
       "      <td>16.49</td>\n",
       "      <td>9.65</td>\n",
       "      <td>14.70</td>\n",
       "      <td>14.72</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.28</td>\n",
       "      <td>15.27</td>\n",
       "      <td>16.57</td>\n",
       "      <td>17.76</td>\n",
       "      <td>17.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.06</td>\n",
       "      <td>7.39</td>\n",
       "      <td>6.90</td>\n",
       "      <td>8.26</td>\n",
       "      <td>6.87</td>\n",
       "      <td>7.81</td>\n",
       "      <td>7.37</td>\n",
       "      <td>7.73</td>\n",
       "      <td>7.87</td>\n",
       "      <td>6.74</td>\n",
       "      <td>2.26</td>\n",
       "      <td>3.45</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.15</td>\n",
       "      <td>2.58</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.40</td>\n",
       "      <td>6.20</td>\n",
       "      <td>14.73</td>\n",
       "      <td>8.17</td>\n",
       "      <td>12.17</td>\n",
       "      <td>12.61</td>\n",
       "      <td>9.49</td>\n",
       "      <td>9.28</td>\n",
       "      <td>13.27</td>\n",
       "      <td>14.56</td>\n",
       "      <td>16.44</td>\n",
       "      <td>15.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.70</td>\n",
       "      <td>11.09</td>\n",
       "      <td>12.45</td>\n",
       "      <td>13.65</td>\n",
       "      <td>12.58</td>\n",
       "      <td>11.74</td>\n",
       "      <td>11.66</td>\n",
       "      <td>13.04</td>\n",
       "      <td>11.70</td>\n",
       "      <td>10.55</td>\n",
       "      <td>5.57</td>\n",
       "      <td>4.55</td>\n",
       "      <td>4.01</td>\n",
       "      <td>7.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.09</td>\n",
       "      <td>3.36</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.83</td>\n",
       "      <td>3.49</td>\n",
       "      <td>18.76</td>\n",
       "      <td>12.37</td>\n",
       "      <td>17.13</td>\n",
       "      <td>17.28</td>\n",
       "      <td>12.72</td>\n",
       "      <td>12.86</td>\n",
       "      <td>17.79</td>\n",
       "      <td>18.91</td>\n",
       "      <td>19.93</td>\n",
       "      <td>19.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.37</td>\n",
       "      <td>6.73</td>\n",
       "      <td>7.17</td>\n",
       "      <td>8.63</td>\n",
       "      <td>7.15</td>\n",
       "      <td>7.37</td>\n",
       "      <td>7.05</td>\n",
       "      <td>7.86</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.23</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2.58</td>\n",
       "      <td>6.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.42</td>\n",
       "      <td>4.30</td>\n",
       "      <td>5.79</td>\n",
       "      <td>5.86</td>\n",
       "      <td>16.02</td>\n",
       "      <td>9.16</td>\n",
       "      <td>13.74</td>\n",
       "      <td>14.00</td>\n",
       "      <td>10.46</td>\n",
       "      <td>10.11</td>\n",
       "      <td>14.58</td>\n",
       "      <td>15.93</td>\n",
       "      <td>17.54</td>\n",
       "      <td>16.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.37</td>\n",
       "      <td>9.04</td>\n",
       "      <td>10.06</td>\n",
       "      <td>11.46</td>\n",
       "      <td>10.13</td>\n",
       "      <td>9.81</td>\n",
       "      <td>9.58</td>\n",
       "      <td>10.69</td>\n",
       "      <td>9.88</td>\n",
       "      <td>8.64</td>\n",
       "      <td>3.02</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.18</td>\n",
       "      <td>5.05</td>\n",
       "      <td>3.36</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.99</td>\n",
       "      <td>17.29</td>\n",
       "      <td>10.54</td>\n",
       "      <td>15.53</td>\n",
       "      <td>15.55</td>\n",
       "      <td>11.21</td>\n",
       "      <td>11.08</td>\n",
       "      <td>16.10</td>\n",
       "      <td>17.39</td>\n",
       "      <td>18.55</td>\n",
       "      <td>17.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.17</td>\n",
       "      <td>9.44</td>\n",
       "      <td>10.84</td>\n",
       "      <td>12.12</td>\n",
       "      <td>11.04</td>\n",
       "      <td>10.25</td>\n",
       "      <td>10.01</td>\n",
       "      <td>11.48</td>\n",
       "      <td>10.10</td>\n",
       "      <td>8.92</td>\n",
       "      <td>4.06</td>\n",
       "      <td>3.11</td>\n",
       "      <td>2.51</td>\n",
       "      <td>5.67</td>\n",
       "      <td>2.40</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3.04</td>\n",
       "      <td>17.65</td>\n",
       "      <td>11.10</td>\n",
       "      <td>15.91</td>\n",
       "      <td>16.02</td>\n",
       "      <td>11.58</td>\n",
       "      <td>11.65</td>\n",
       "      <td>16.60</td>\n",
       "      <td>17.77</td>\n",
       "      <td>18.94</td>\n",
       "      <td>18.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.45</td>\n",
       "      <td>10.75</td>\n",
       "      <td>12.06</td>\n",
       "      <td>13.40</td>\n",
       "      <td>12.20</td>\n",
       "      <td>11.56</td>\n",
       "      <td>11.40</td>\n",
       "      <td>12.64</td>\n",
       "      <td>11.58</td>\n",
       "      <td>10.46</td>\n",
       "      <td>5.39</td>\n",
       "      <td>5.24</td>\n",
       "      <td>3.22</td>\n",
       "      <td>7.40</td>\n",
       "      <td>2.83</td>\n",
       "      <td>5.79</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.49</td>\n",
       "      <td>18.41</td>\n",
       "      <td>11.88</td>\n",
       "      <td>17.06</td>\n",
       "      <td>16.91</td>\n",
       "      <td>12.25</td>\n",
       "      <td>12.19</td>\n",
       "      <td>17.40</td>\n",
       "      <td>18.63</td>\n",
       "      <td>19.45</td>\n",
       "      <td>19.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.46</td>\n",
       "      <td>11.10</td>\n",
       "      <td>12.05</td>\n",
       "      <td>13.27</td>\n",
       "      <td>12.32</td>\n",
       "      <td>11.89</td>\n",
       "      <td>11.47</td>\n",
       "      <td>12.84</td>\n",
       "      <td>11.45</td>\n",
       "      <td>10.49</td>\n",
       "      <td>5.28</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.34</td>\n",
       "      <td>6.20</td>\n",
       "      <td>3.49</td>\n",
       "      <td>5.86</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.04</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.69</td>\n",
       "      <td>10.64</td>\n",
       "      <td>15.04</td>\n",
       "      <td>15.23</td>\n",
       "      <td>10.41</td>\n",
       "      <td>10.90</td>\n",
       "      <td>15.81</td>\n",
       "      <td>16.96</td>\n",
       "      <td>18.07</td>\n",
       "      <td>16.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.78</td>\n",
       "      <td>16.96</td>\n",
       "      <td>14.77</td>\n",
       "      <td>15.07</td>\n",
       "      <td>15.47</td>\n",
       "      <td>17.32</td>\n",
       "      <td>16.38</td>\n",
       "      <td>15.63</td>\n",
       "      <td>16.39</td>\n",
       "      <td>17.07</td>\n",
       "      <td>15.82</td>\n",
       "      <td>17.12</td>\n",
       "      <td>16.49</td>\n",
       "      <td>14.73</td>\n",
       "      <td>18.76</td>\n",
       "      <td>16.02</td>\n",
       "      <td>17.29</td>\n",
       "      <td>17.65</td>\n",
       "      <td>18.41</td>\n",
       "      <td>16.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.80</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.89</td>\n",
       "      <td>7.81</td>\n",
       "      <td>7.52</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.76</td>\n",
       "      <td>4.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.96</td>\n",
       "      <td>11.07</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.75</td>\n",
       "      <td>9.60</td>\n",
       "      <td>11.51</td>\n",
       "      <td>10.64</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.82</td>\n",
       "      <td>11.07</td>\n",
       "      <td>9.00</td>\n",
       "      <td>10.61</td>\n",
       "      <td>9.65</td>\n",
       "      <td>8.17</td>\n",
       "      <td>12.37</td>\n",
       "      <td>9.16</td>\n",
       "      <td>10.54</td>\n",
       "      <td>11.10</td>\n",
       "      <td>11.88</td>\n",
       "      <td>10.64</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.46</td>\n",
       "      <td>6.20</td>\n",
       "      <td>3.62</td>\n",
       "      <td>2.35</td>\n",
       "      <td>6.44</td>\n",
       "      <td>8.02</td>\n",
       "      <td>9.22</td>\n",
       "      <td>9.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.82</td>\n",
       "      <td>14.50</td>\n",
       "      <td>12.02</td>\n",
       "      <td>12.27</td>\n",
       "      <td>12.59</td>\n",
       "      <td>14.71</td>\n",
       "      <td>13.81</td>\n",
       "      <td>12.88</td>\n",
       "      <td>13.92</td>\n",
       "      <td>14.45</td>\n",
       "      <td>13.62</td>\n",
       "      <td>14.83</td>\n",
       "      <td>14.70</td>\n",
       "      <td>12.17</td>\n",
       "      <td>17.13</td>\n",
       "      <td>13.74</td>\n",
       "      <td>15.53</td>\n",
       "      <td>15.91</td>\n",
       "      <td>17.06</td>\n",
       "      <td>15.04</td>\n",
       "      <td>4.30</td>\n",
       "      <td>6.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.91</td>\n",
       "      <td>7.20</td>\n",
       "      <td>6.83</td>\n",
       "      <td>3.11</td>\n",
       "      <td>4.16</td>\n",
       "      <td>6.69</td>\n",
       "      <td>5.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.48</td>\n",
       "      <td>14.90</td>\n",
       "      <td>12.51</td>\n",
       "      <td>12.91</td>\n",
       "      <td>13.15</td>\n",
       "      <td>15.25</td>\n",
       "      <td>14.28</td>\n",
       "      <td>13.38</td>\n",
       "      <td>14.41</td>\n",
       "      <td>14.98</td>\n",
       "      <td>13.82</td>\n",
       "      <td>15.21</td>\n",
       "      <td>14.72</td>\n",
       "      <td>12.61</td>\n",
       "      <td>17.28</td>\n",
       "      <td>14.00</td>\n",
       "      <td>15.55</td>\n",
       "      <td>16.02</td>\n",
       "      <td>16.91</td>\n",
       "      <td>15.23</td>\n",
       "      <td>2.89</td>\n",
       "      <td>6.20</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.76</td>\n",
       "      <td>6.26</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.88</td>\n",
       "      <td>5.16</td>\n",
       "      <td>5.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.54</td>\n",
       "      <td>13.19</td>\n",
       "      <td>11.63</td>\n",
       "      <td>12.49</td>\n",
       "      <td>12.19</td>\n",
       "      <td>13.83</td>\n",
       "      <td>12.88</td>\n",
       "      <td>12.66</td>\n",
       "      <td>12.99</td>\n",
       "      <td>13.09</td>\n",
       "      <td>10.16</td>\n",
       "      <td>11.52</td>\n",
       "      <td>10.49</td>\n",
       "      <td>9.49</td>\n",
       "      <td>12.72</td>\n",
       "      <td>10.46</td>\n",
       "      <td>11.21</td>\n",
       "      <td>11.58</td>\n",
       "      <td>12.25</td>\n",
       "      <td>10.41</td>\n",
       "      <td>7.81</td>\n",
       "      <td>3.62</td>\n",
       "      <td>7.20</td>\n",
       "      <td>6.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.31</td>\n",
       "      <td>6.84</td>\n",
       "      <td>8.68</td>\n",
       "      <td>9.23</td>\n",
       "      <td>9.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.54</td>\n",
       "      <td>12.41</td>\n",
       "      <td>10.59</td>\n",
       "      <td>11.42</td>\n",
       "      <td>11.14</td>\n",
       "      <td>12.94</td>\n",
       "      <td>12.06</td>\n",
       "      <td>11.57</td>\n",
       "      <td>12.27</td>\n",
       "      <td>12.43</td>\n",
       "      <td>9.89</td>\n",
       "      <td>11.47</td>\n",
       "      <td>10.28</td>\n",
       "      <td>9.28</td>\n",
       "      <td>12.86</td>\n",
       "      <td>10.11</td>\n",
       "      <td>11.08</td>\n",
       "      <td>11.65</td>\n",
       "      <td>12.19</td>\n",
       "      <td>10.90</td>\n",
       "      <td>7.52</td>\n",
       "      <td>2.35</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.26</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.25</td>\n",
       "      <td>8.27</td>\n",
       "      <td>8.77</td>\n",
       "      <td>9.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.96</td>\n",
       "      <td>15.57</td>\n",
       "      <td>13.05</td>\n",
       "      <td>13.41</td>\n",
       "      <td>13.62</td>\n",
       "      <td>15.86</td>\n",
       "      <td>14.96</td>\n",
       "      <td>13.92</td>\n",
       "      <td>15.13</td>\n",
       "      <td>15.67</td>\n",
       "      <td>14.44</td>\n",
       "      <td>15.87</td>\n",
       "      <td>15.27</td>\n",
       "      <td>13.27</td>\n",
       "      <td>17.79</td>\n",
       "      <td>14.58</td>\n",
       "      <td>16.10</td>\n",
       "      <td>16.60</td>\n",
       "      <td>17.40</td>\n",
       "      <td>15.81</td>\n",
       "      <td>2.89</td>\n",
       "      <td>6.44</td>\n",
       "      <td>3.11</td>\n",
       "      <td>2.25</td>\n",
       "      <td>6.84</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>4.65</td>\n",
       "      <td>6.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.19</td>\n",
       "      <td>16.54</td>\n",
       "      <td>14.18</td>\n",
       "      <td>14.39</td>\n",
       "      <td>14.90</td>\n",
       "      <td>16.84</td>\n",
       "      <td>15.90</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.89</td>\n",
       "      <td>16.65</td>\n",
       "      <td>15.75</td>\n",
       "      <td>17.05</td>\n",
       "      <td>16.57</td>\n",
       "      <td>14.56</td>\n",
       "      <td>18.91</td>\n",
       "      <td>15.93</td>\n",
       "      <td>17.39</td>\n",
       "      <td>17.77</td>\n",
       "      <td>18.63</td>\n",
       "      <td>16.96</td>\n",
       "      <td>2.18</td>\n",
       "      <td>8.02</td>\n",
       "      <td>4.16</td>\n",
       "      <td>2.88</td>\n",
       "      <td>8.68</td>\n",
       "      <td>8.27</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.86</td>\n",
       "      <td>3.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.40</td>\n",
       "      <td>18.48</td>\n",
       "      <td>16.35</td>\n",
       "      <td>16.62</td>\n",
       "      <td>17.07</td>\n",
       "      <td>18.82</td>\n",
       "      <td>17.95</td>\n",
       "      <td>17.16</td>\n",
       "      <td>17.94</td>\n",
       "      <td>18.67</td>\n",
       "      <td>17.31</td>\n",
       "      <td>18.68</td>\n",
       "      <td>17.76</td>\n",
       "      <td>16.44</td>\n",
       "      <td>19.93</td>\n",
       "      <td>17.54</td>\n",
       "      <td>18.55</td>\n",
       "      <td>18.94</td>\n",
       "      <td>19.45</td>\n",
       "      <td>18.07</td>\n",
       "      <td>2.76</td>\n",
       "      <td>9.22</td>\n",
       "      <td>6.69</td>\n",
       "      <td>5.16</td>\n",
       "      <td>9.23</td>\n",
       "      <td>8.77</td>\n",
       "      <td>4.65</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.32</td>\n",
       "      <td>17.22</td>\n",
       "      <td>15.31</td>\n",
       "      <td>15.41</td>\n",
       "      <td>16.14</td>\n",
       "      <td>17.54</td>\n",
       "      <td>16.60</td>\n",
       "      <td>16.13</td>\n",
       "      <td>16.38</td>\n",
       "      <td>17.21</td>\n",
       "      <td>16.40</td>\n",
       "      <td>17.43</td>\n",
       "      <td>17.11</td>\n",
       "      <td>15.18</td>\n",
       "      <td>19.05</td>\n",
       "      <td>16.60</td>\n",
       "      <td>17.90</td>\n",
       "      <td>18.03</td>\n",
       "      <td>19.05</td>\n",
       "      <td>16.99</td>\n",
       "      <td>4.28</td>\n",
       "      <td>9.06</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.46</td>\n",
       "      <td>9.46</td>\n",
       "      <td>9.44</td>\n",
       "      <td>6.26</td>\n",
       "      <td>3.60</td>\n",
       "      <td>5.63</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      0      0      0      0      0      0      0      0      0  \\\n",
       "0   0.00   4.96   1.67   2.36   0.98   4.53   4.24   1.62   5.31   4.96   \n",
       "0   4.96   0.00   4.53   4.94   5.30   1.90   1.22   4.41   1.95   1.36   \n",
       "0   1.67   4.53   0.00   1.85   1.71   4.51   3.80   1.33   4.66   4.75   \n",
       "0   2.36   4.94   1.85   0.00   2.82   4.56   4.02   1.65   4.61   5.22   \n",
       "0   0.98   5.30   1.71   2.82   0.00   5.03   4.66   1.81   5.80   5.41   \n",
       "0   4.53   1.90   4.51   4.56   5.03   0.00   1.75   4.10   2.74   2.15   \n",
       "0   4.24   1.22   3.80   4.02   4.66   1.75   0.00   3.65   1.72   1.76   \n",
       "0   1.62   4.41   1.33   1.65   1.81   4.10   3.65   0.00   4.68   4.78   \n",
       "0   5.31   1.95   4.66   4.61   5.80   2.74   1.72   4.68   0.00   2.15   \n",
       "0   4.96   1.36   4.75   5.22   5.41   2.15   1.76   4.78   2.15   0.00   \n",
       "1   8.02   7.66   7.82   9.27   7.77   8.21   7.93   8.52   8.44   7.16   \n",
       "1   9.15   7.98   9.06  10.35   9.06   8.57   8.38   9.67   8.70   7.27   \n",
       "1   9.60   8.48   9.25  10.66   9.34   9.19   8.95   9.90   9.30   8.11   \n",
       "1   7.06   7.39   6.90   8.26   6.87   7.81   7.37   7.73   7.87   6.74   \n",
       "1  12.70  11.09  12.45  13.65  12.58  11.74  11.66  13.04  11.70  10.55   \n",
       "1   7.37   6.73   7.17   8.63   7.15   7.37   7.05   7.86   7.60   6.23   \n",
       "1  10.37   9.04  10.06  11.46  10.13   9.81   9.58  10.69   9.88   8.64   \n",
       "1  11.17   9.44  10.84  12.12  11.04  10.25  10.01  11.48  10.10   8.92   \n",
       "1  12.45  10.75  12.06  13.40  12.20  11.56  11.40  12.64  11.58  10.46   \n",
       "1  12.46  11.10  12.05  13.27  12.32  11.89  11.47  12.84  11.45  10.49   \n",
       "2  15.78  16.96  14.77  15.07  15.47  17.32  16.38  15.63  16.39  17.07   \n",
       "2   9.96  11.07   9.00   9.75   9.60  11.51  10.64   9.99  10.82  11.07   \n",
       "2  12.82  14.50  12.02  12.27  12.59  14.71  13.81  12.88  13.92  14.45   \n",
       "2  13.48  14.90  12.51  12.91  13.15  15.25  14.28  13.38  14.41  14.98   \n",
       "2  12.54  13.19  11.63  12.49  12.19  13.83  12.88  12.66  12.99  13.09   \n",
       "2  11.54  12.41  10.59  11.42  11.14  12.94  12.06  11.57  12.27  12.43   \n",
       "2  13.96  15.57  13.05  13.41  13.62  15.86  14.96  13.92  15.13  15.67   \n",
       "2  15.19  16.54  14.18  14.39  14.90  16.84  15.90  15.00  15.89  16.65   \n",
       "2  17.40  18.48  16.35  16.62  17.07  18.82  17.95  17.16  17.94  18.67   \n",
       "2  16.32  17.22  15.31  15.41  16.14  17.54  16.60  16.13  16.38  17.21   \n",
       "\n",
       "       1      1      1      1      1      1      1      1      1      1  \\\n",
       "0   8.02   9.15   9.60   7.06  12.70   7.37  10.37  11.17  12.45  12.46   \n",
       "0   7.66   7.98   8.48   7.39  11.09   6.73   9.04   9.44  10.75  11.10   \n",
       "0   7.82   9.06   9.25   6.90  12.45   7.17  10.06  10.84  12.06  12.05   \n",
       "0   9.27  10.35  10.66   8.26  13.65   8.63  11.46  12.12  13.40  13.27   \n",
       "0   7.77   9.06   9.34   6.87  12.58   7.15  10.13  11.04  12.20  12.32   \n",
       "0   8.21   8.57   9.19   7.81  11.74   7.37   9.81  10.25  11.56  11.89   \n",
       "0   7.93   8.38   8.95   7.37  11.66   7.05   9.58  10.01  11.40  11.47   \n",
       "0   8.52   9.67   9.90   7.73  13.04   7.86  10.69  11.48  12.64  12.84   \n",
       "0   8.44   8.70   9.30   7.87  11.70   7.60   9.88  10.10  11.58  11.45   \n",
       "0   7.16   7.27   8.11   6.74  10.55   6.23   8.64   8.92  10.46  10.49   \n",
       "1   0.00   2.52   2.33   2.26   5.57   1.54   3.02   4.06   5.39   5.28   \n",
       "1   2.52   0.00   3.07   3.45   4.55   2.61   3.10   3.11   5.24   4.51   \n",
       "1   2.33   3.07   0.00   4.35   4.01   2.83   1.18   2.51   3.22   4.34   \n",
       "1   2.26   3.45   4.35   0.00   7.15   2.58   5.05   5.67   7.40   6.20   \n",
       "1   5.57   4.55   4.01   7.15   0.00   6.09   3.36   2.40   2.83   3.49   \n",
       "1   1.54   2.61   2.83   2.58   6.09   0.00   3.42   4.30   5.79   5.86   \n",
       "1   3.02   3.10   1.18   5.05   3.36   3.42   0.00   1.80   2.50   3.99   \n",
       "1   4.06   3.11   2.51   5.67   2.40   4.30   1.80   0.00   2.70   3.04   \n",
       "1   5.39   5.24   3.22   7.40   2.83   5.79   2.50   2.70   0.00   4.49   \n",
       "1   5.28   4.51   4.34   6.20   3.49   5.86   3.99   3.04   4.49   0.00   \n",
       "2  15.82  17.12  16.49  14.73  18.76  16.02  17.29  17.65  18.41  16.69   \n",
       "2   9.00  10.61   9.65   8.17  12.37   9.16  10.54  11.10  11.88  10.64   \n",
       "2  13.62  14.83  14.70  12.17  17.13  13.74  15.53  15.91  17.06  15.04   \n",
       "2  13.82  15.21  14.72  12.61  17.28  14.00  15.55  16.02  16.91  15.23   \n",
       "2  10.16  11.52  10.49   9.49  12.72  10.46  11.21  11.58  12.25  10.41   \n",
       "2   9.89  11.47  10.28   9.28  12.86  10.11  11.08  11.65  12.19  10.90   \n",
       "2  14.44  15.87  15.27  13.27  17.79  14.58  16.10  16.60  17.40  15.81   \n",
       "2  15.75  17.05  16.57  14.56  18.91  15.93  17.39  17.77  18.63  16.96   \n",
       "2  17.31  18.68  17.76  16.44  19.93  17.54  18.55  18.94  19.45  18.07   \n",
       "2  16.40  17.43  17.11  15.18  19.05  16.60  17.90  18.03  19.05  16.99   \n",
       "\n",
       "       2      2      2      2      2      2      2      2      2      2  \n",
       "0  15.78   9.96  12.82  13.48  12.54  11.54  13.96  15.19  17.40  16.32  \n",
       "0  16.96  11.07  14.50  14.90  13.19  12.41  15.57  16.54  18.48  17.22  \n",
       "0  14.77   9.00  12.02  12.51  11.63  10.59  13.05  14.18  16.35  15.31  \n",
       "0  15.07   9.75  12.27  12.91  12.49  11.42  13.41  14.39  16.62  15.41  \n",
       "0  15.47   9.60  12.59  13.15  12.19  11.14  13.62  14.90  17.07  16.14  \n",
       "0  17.32  11.51  14.71  15.25  13.83  12.94  15.86  16.84  18.82  17.54  \n",
       "0  16.38  10.64  13.81  14.28  12.88  12.06  14.96  15.90  17.95  16.60  \n",
       "0  15.63   9.99  12.88  13.38  12.66  11.57  13.92  15.00  17.16  16.13  \n",
       "0  16.39  10.82  13.92  14.41  12.99  12.27  15.13  15.89  17.94  16.38  \n",
       "0  17.07  11.07  14.45  14.98  13.09  12.43  15.67  16.65  18.67  17.21  \n",
       "1  15.82   9.00  13.62  13.82  10.16   9.89  14.44  15.75  17.31  16.40  \n",
       "1  17.12  10.61  14.83  15.21  11.52  11.47  15.87  17.05  18.68  17.43  \n",
       "1  16.49   9.65  14.70  14.72  10.49  10.28  15.27  16.57  17.76  17.11  \n",
       "1  14.73   8.17  12.17  12.61   9.49   9.28  13.27  14.56  16.44  15.18  \n",
       "1  18.76  12.37  17.13  17.28  12.72  12.86  17.79  18.91  19.93  19.05  \n",
       "1  16.02   9.16  13.74  14.00  10.46  10.11  14.58  15.93  17.54  16.60  \n",
       "1  17.29  10.54  15.53  15.55  11.21  11.08  16.10  17.39  18.55  17.90  \n",
       "1  17.65  11.10  15.91  16.02  11.58  11.65  16.60  17.77  18.94  18.03  \n",
       "1  18.41  11.88  17.06  16.91  12.25  12.19  17.40  18.63  19.45  19.05  \n",
       "1  16.69  10.64  15.04  15.23  10.41  10.90  15.81  16.96  18.07  16.99  \n",
       "2   0.00   7.80   4.30   2.89   7.81   7.52   2.89   2.18   2.76   4.28  \n",
       "2   7.80   0.00   6.46   6.20   3.62   2.35   6.44   8.02   9.22   9.06  \n",
       "2   4.30   6.46   0.00   2.91   7.20   6.83   3.11   4.16   6.69   5.75  \n",
       "2   2.89   6.20   2.91   0.00   6.76   6.26   2.25   2.88   5.16   5.46  \n",
       "2   7.81   3.62   7.20   6.76   0.00   2.31   6.84   8.68   9.23   9.46  \n",
       "2   7.52   2.35   6.83   6.26   2.31   0.00   6.25   8.27   8.77   9.44  \n",
       "2   2.89   6.44   3.11   2.25   6.84   6.25   0.00   3.60   4.65   6.26  \n",
       "2   2.18   8.02   4.16   2.88   8.68   8.27   3.60   0.00   3.86   3.60  \n",
       "2   2.76   9.22   6.69   5.16   9.23   8.77   4.65   3.86   0.00   5.63  \n",
       "2   4.28   9.06   5.75   5.46   9.46   9.44   6.26   3.60   5.63   0.00  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "matrix = [ [ \"-1\" ] * len(test_labels) for i in range(len(test_labels)) ]\n",
    "for i, (emb1, label1) in enumerate(zip(test_embeddings, test_labels)):\n",
    "    for j, (emb2, label2) in enumerate(zip(test_embeddings, test_labels)):\n",
    "        euc_dist = torch.nn.functional.pairwise_distance(emb1, emb2).item()\n",
    "        matrix[i][j] = \"{0:.2f}\".format(euc_dist)\n",
    "\n",
    "pandas.options.display.max_columns = 30\n",
    "df = pandas.DataFrame(matrix, columns = test_labels.tolist())\n",
    "df.index = test_labels.tolist()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c892920400>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "correct_dists = [ ]\n",
    "incorrect_dists = [ ]\n",
    "for i, (emb1, label1) in enumerate(zip(test_embeddings, test_labels)):\n",
    "    for j, (emb2, label2) in enumerate(zip(test_embeddings, test_labels)):\n",
    "        euc_dist = torch.nn.functional.pairwise_distance(emb1, emb2).item()\n",
    "        if label1 == label2: correct_dists.append(euc_dist)\n",
    "        else: incorrect_dists.append(euc_dist)\n",
    "\n",
    "plt.figure(figsize = (10, 2))\n",
    "plt.ylim(-1, 2)\n",
    "plt.plot(correct_dists, numpy.zeros_like(correct_dists), 'x')\n",
    "plt.plot(incorrect_dists, numpy.ones_like(incorrect_dists), 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c892926a70>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correct_dists = [ ]\n",
    "incorrect_dists = [ ]\n",
    "for i, (emb1, label1) in enumerate(zip(test_embeddings, test_labels)):\n",
    "    for j, (emb2, label2) in enumerate(zip(test_embeddings, test_labels)):\n",
    "        cos_dist = torch.nn.functional.cosine_similarity(emb1, emb2, dim = 0).item()\n",
    "        if label1 == label2: correct_dists.append(cos_dist)\n",
    "        else: incorrect_dists.append(cos_dist)\n",
    "\n",
    "plt.figure(figsize = (10, 2))\n",
    "plt.ylim(-1, 2)\n",
    "plt.plot(correct_dists, numpy.zeros_like(correct_dists), 'x')\n",
    "plt.plot(incorrect_dists, numpy.ones_like(incorrect_dists), 'x')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
