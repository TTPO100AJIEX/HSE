{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "\n",
        "subj = \"phase2/Subj7\"\n",
        "\n",
        "UNIQUE_VALUES_THRESHOLD = 200\n",
        "\n",
        "exp = \"exp_reduced_flow\"\n",
        "os.makedirs(f\"{subj}/{exp}\", exist_ok = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import typing\n",
        "\n",
        "import mne\n",
        "import numpy\n",
        "import pandas\n",
        "import IPython.display\n",
        "import sklearn.metrics\n",
        "import matplotlib.colors\n",
        "import sklearn.preprocessing\n",
        "import sklearn.decomposition\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import SDA.topology\n",
        "import SDA.analytics\n",
        "import SDA.clustquality\n",
        "import SDA.stageprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = mne.read_epochs(f\"{subj}/src/epochs_filt_rr-epo.fif\").drop_channels(ch_names = [ 'IVEOG', 'IHEOG' ])\n",
        "N_STAGES = int(numpy.loadtxt(f\"{subj}/src/n_stages.txt\"))\n",
        "print('Stages: ', N_STAGES)\n",
        "\n",
        "epochs.average().plot_joint().savefig(f\"{subj}/{exp}/eeg.svg\")\n",
        "data = epochs.get_data(copy = True)\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "per_channel_folder = f\"{subj}/{exp}/features/per_channel\"\n",
        "os.makedirs(per_channel_folder, exist_ok = True)\n",
        "per_channel_extractor = SDA.topology.PerChannelFeatureExtractor(n_jobs = -1, folder = per_channel_folder, reduced = True)\n",
        "per_channel_features = per_channel_extractor.extract(data)\n",
        "\n",
        "dissimilarity_folder = f\"{subj}/{exp}/features/dissimilarity\"\n",
        "os.makedirs(dissimilarity_folder, exist_ok = True)\n",
        "dissimilarity_extractor = SDA.topology.DissimilarityFeatureExtractor(n_jobs = -1, folder = dissimilarity_folder, reduced = True)\n",
        "dissimilarity_features = dissimilarity_extractor.extract(data)\n",
        "\n",
        "overall_folder = f\"{subj}/{exp}/features/overall\"\n",
        "os.makedirs(overall_folder, exist_ok = True)\n",
        "overall_extractor = SDA.topology.OverallFeatureExtractor(n_jobs = -1, folder = overall_folder, reduced = True)\n",
        "overall_features = overall_extractor.extract(data)\n",
        "\n",
        "all_features = pandas.concat([\n",
        "    per_channel_features,\n",
        "    dissimilarity_features,\n",
        "    overall_features\n",
        "], axis = 1)\n",
        "\n",
        "all_features.to_feather(f\"{subj}/{exp}/features/all_features.feather\")\n",
        "display(all_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = sklearn.preprocessing.StandardScaler().fit_transform(all_features)\n",
        "features = pandas.DataFrame(features, columns = all_features.columns)\n",
        "\n",
        "features.to_feather(f\"{subj}/{exp}/features/features.feather\")\n",
        "display(features)\n",
        "\n",
        "numpy.save(f\"{subj}/{exp}/features/features.npy\", features.to_numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs(f\"{subj}/{exp}/qsda\", exist_ok = True)\n",
        "qsda = SDA.QSDA(\n",
        "    n_jobs = 1,\n",
        "    qsda_n_jobs = 15,\n",
        "    scores_folder = f\"{subj}/{exp}/qsda\",\n",
        "\n",
        "    threshold = 1150,\n",
        "    min_unique_values = UNIQUE_VALUES_THRESHOLD\n",
        ")\n",
        "best_features, scores = qsda.select(features)\n",
        "\n",
        "best_features.to_feather(f\"{subj}/{exp}/qsda/best_features.feather\")\n",
        "numpy.save(f\"{subj}/{exp}/qsda/best_features.npy\", features.to_numpy())\n",
        "display(best_features)\n",
        "display(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Estimator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze(all_features: pandas.DataFrame, n_components: int, folder: str):\n",
        "    folder = f\"{subj}/{exp}/results/{folder}\"\n",
        "    os.makedirs(folder, exist_ok = True)\n",
        "\n",
        "    # Scale features\n",
        "    all_features = sklearn.preprocessing.StandardScaler().fit_transform(all_features)\n",
        "    print(all_features.shape)\n",
        "    numpy.save(f\"{folder}/all_features.npy\", all_features)\n",
        "    numpy.savetxt(f\"{folder}/all_features_shape.txt\", all_features.shape)\n",
        "\n",
        "    # PCA\n",
        "    pca = sklearn.decomposition.PCA(n_components = n_components, svd_solver = \"full\", random_state = 42)\n",
        "    pca_features = pca.fit_transform(all_features)\n",
        "    print(pca_features.shape)\n",
        "    numpy.save(f\"{folder}/pca_features.npy\", pca_features)\n",
        "    numpy.savetxt(f\"{folder}/pca_features_shape.txt\", pca_features.shape)\n",
        "    \n",
        "    print('Explained variance', round(pca.explained_variance_ratio_.sum(), 2))\n",
        "    print([ round(x, 3) for x in pca.explained_variance_ratio_ ])\n",
        "    numpy.savetxt(f\"{folder}/explained_variance.txt\", [ pca.explained_variance_ratio_.sum() ])\n",
        "    numpy.savetxt(f\"{folder}/explained_variance_ratios.txt\", pca.explained_variance_ratio_)\n",
        "\n",
        "    # SDA\n",
        "    sda = SDA.SDA(n_jobs = -1, scale = False, verbose = True)\n",
        "    results, df_st_edges = sda.apply(pca_features)\n",
        "    \n",
        "    results.to_csv(f\"{folder}/results.csv\")\n",
        "    df_st_edges.to_csv(f\"{folder}/df_st_edges.csv\")\n",
        "\n",
        "    # Analyze\n",
        "    best_results = SDA.analytics.best_results(results, key = 'Avg-Silh')\n",
        "    best_results.to_csv(f\"{folder}/best_results.csv\")\n",
        "    \n",
        "    best_result = SDA.analytics.best_result(results, key = 'Avg-Silh', n_stages = N_STAGES)\n",
        "    best_result_df = pandas.DataFrame([ best_result ])\n",
        "    best_result_df.to_csv(f\"{folder}/best_result.csv\")\n",
        "    display(best_result_df)\n",
        "    \n",
        "    best_edges = numpy.array(best_result['St_edges'])\n",
        "    numpy.savetxt(f\"{folder}/best_edges.txt\", best_edges, fmt = \"%d\", newline = ' ')\n",
        "    display(best_edges)\n",
        "\n",
        "    stage_timing = SDA.analytics.stage_timing(best_edges, epochs)\n",
        "    stage_timing.to_csv(f\"{folder}/stage_timing.csv\")\n",
        "    display(stage_timing)\n",
        "    \n",
        "    SDA.analytics.plot_stats(pca_features, epochs, best_result, df_st_edges).savefig(f\"{folder}/stats.svg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Традиционные признаки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_traditional():\n",
        "    df_ft_psd_loc_db = pandas.read_feather(f'{subj}/src/df_ft_psd_loc_db.feather')\n",
        "    df_ft_psd_ind_loc_log = pandas.read_feather(f'{subj}/src/df_ft_psd_ind_loc_log.feather')\n",
        "    df_ft_coh_ind_loc = pandas.read_feather(f'{subj}/src/df_ft_coh_ind_loc.feather')\n",
        "    df_ft_plv_ind_loc = pandas.read_feather(f'{subj}/src/df_ft_plv_ind_loc.feather')\n",
        "\n",
        "    result =  pandas.concat([\n",
        "        df_ft_psd_loc_db,\n",
        "        df_ft_psd_ind_loc_log,\n",
        "        df_ft_coh_ind_loc,\n",
        "        df_ft_plv_ind_loc\n",
        "    ], axis = 1)\n",
        "\n",
        "    if subj == \"Subj2\":\n",
        "        result = result[:-2]\n",
        "    return result\n",
        "\n",
        "analyze(read_traditional(), 15, \"traditional\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Топологические признаки + QSDA + PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyze(best_features, 15, \"best_topological\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Традиционные и топологические вместе + PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_features = pandas.concat([ read_traditional(), best_features ], axis = 1)\n",
        "analyze(combined_features, 15, \"combined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Information value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_edges = numpy.loadtxt(f\"{subj}/{exp}/results/best_topological/best_edges.txt\").astype(numpy.int32)\n",
        "print(result_edges)\n",
        "\n",
        "if os.path.exists(f\"{subj}/{exp}/IV/IV.csv\"):\n",
        "    ivs = pandas.read_csv(f\"{subj}/{exp}/IV/IV.csv\")\n",
        "else:\n",
        "    _, labels = SDA.stageprocess.form_stage_bands(result_edges)\n",
        "    ivs = SDA.analytics.IV.calc_IV_clust(features, labels)\n",
        "\n",
        "    os.makedirs(f\"{subj}/{exp}/IV\", exist_ok = True)\n",
        "    ivs.to_csv(f\"{subj}/{exp}/IV/IV.csv\", index = False)\n",
        "\n",
        "ivs[\"normalized_iv\"] = sklearn.preprocessing.MinMaxScaler().fit_transform(ivs[[\"IV\"]])\n",
        "display(ivs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iv_qsda_scores = scores.join(ivs, lsuffix = \"name\", rsuffix = \"Feature\")\n",
        "iv_qsda_scores = iv_qsda_scores[[\"name\", \"unique_values\", \"score\", \"normalized_score\", \"IV\", \"normalized_iv\"]]\n",
        "\n",
        "iv_qsda_scores[\"score\"] = iv_qsda_scores[\"score\"] * (iv_qsda_scores[\"unique_values\"] >= UNIQUE_VALUES_THRESHOLD)\n",
        "iv_qsda_scores[\"normalized_score\"] = iv_qsda_scores[\"normalized_score\"] * (iv_qsda_scores[\"unique_values\"] >= UNIQUE_VALUES_THRESHOLD)\n",
        "\n",
        "iv_qsda_scores = iv_qsda_scores.sort_values(by = 'score', ascending = False)\n",
        "iv_qsda_scores[\"QSDA_IDX\"] = numpy.arange(1, iv_qsda_scores.shape[0] + 1)\n",
        "\n",
        "iv_qsda_scores = iv_qsda_scores.sort_values(by = 'IV', ascending = False)\n",
        "iv_qsda_scores[\"IV_IDX\"] = numpy.arange(1, iv_qsda_scores.shape[0] + 1)\n",
        "\n",
        "iv_qsda_scores[\"IDX_DIFF\"] = iv_qsda_scores[\"QSDA_IDX\"] - iv_qsda_scores[\"IV_IDX\"]\n",
        "\n",
        "iv_qsda_scores = iv_qsda_scores.sort_index()\n",
        "display(iv_qsda_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iv_qsda_scores[\"IDX_DIFF\"].hist(bins = 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vec_div(vec):\n",
        "    mat1 = numpy.repeat(vec.reshape(-1, 1), repeats = len(vec), axis = 1)\n",
        "    mat2 = numpy.repeat(vec.reshape(1, -1), repeats = len(vec), axis = 0)\n",
        "    return numpy.sign(mat1 - mat2)\n",
        "\n",
        "qsda = vec_div(iv_qsda_scores[\"score\"].to_numpy())\n",
        "iv = vec_div(iv_qsda_scores[\"IV\"].to_numpy())\n",
        "\n",
        "correct = ((qsda == iv).sum() - iv_qsda_scores.shape[0]) / 2\n",
        "incorrect = ((qsda != iv).sum() - iv_qsda_scores.shape[0]) / 2\n",
        "\n",
        "print(correct / 1e6, incorrect / 1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sourceFilter(row):\n",
        "    if row['Source'].startswith('dissim'):\n",
        "        return 'dissim'\n",
        "    return row['Source']\n",
        "\n",
        "iv_qsda_scores[[\"Source\", \"Algorithm\", \"Dimension\", \"Stat\"]] = iv_qsda_scores['name'].str.split(' ', expand = True)\n",
        "iv_qsda_scores[\"Source\"] = iv_qsda_scores.apply(sourceFilter, axis = 1)\n",
        "iv_qsda_scores.to_csv(f\"{subj}/{exp}/IV/iv_qsda_scores.csv\", index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def SourceRenamer(initial: str) -> str:\n",
        "    if initial == 'dissim':\n",
        "        return 'Correlations analysis'\n",
        "    if initial == 'overall':\n",
        "        return 'Overall processing'\n",
        "    ind = initial.find('-') + 1\n",
        "    comp = int(initial[ind:])\n",
        "    return f'Comp. {comp} (ch. {epochs.ch_names[comp]})'\n",
        "\n",
        "def AlgorithmRenamer(row):\n",
        "    return {\n",
        "        'amplitude-betti-1':        'Betti amp. with p=1',\n",
        "        'amplitude-betti-2':        'Betti amp. with p=2',\n",
        "\n",
        "        'amplitude-bottleneck':     'Bottleneck amp.',\n",
        "        'amplitude-wasserstein-1':  'Wasserstein amp. with p=1',\n",
        "        'amplitude-wasserstein-2':  'Wasserstein amp. with p=2',\n",
        "        \n",
        "        'amplitude-landscape-1-1':  'Pers. landscape amp. with p=1',\n",
        "        'amplitude-landscape-1-2':  'Pers. landscape amp. with p=1',\n",
        "        'amplitude-landscape-2-1':  'Pers. landscape amp. with p=2',\n",
        "        'amplitude-landscape-2-2':  'Pers. landscape amp. with p=2',\n",
        "        \n",
        "        'amplitude-silhouette-1-1': 'Pers. silhouette amp. with p=1',\n",
        "        'amplitude-silhouette-1-2': 'Pers. silhouette amp. with p=1',\n",
        "        'amplitude-silhouette-2-1': 'Pers. silhouette amp. with p=2',\n",
        "        'amplitude-silhouette-2-2': 'Pers. silhouette amp. with p=2',\n",
        "\n",
        "        'bd2':                      'Stat. char. of (b + d) / 2',\n",
        "        'life':                     'Stat. char. of lifetimes',\n",
        "        'betti':                    'Stat. char. of Betti curves',\n",
        "        'landscape':                'Stat. char. of pers. landscape',\n",
        "        'silhouette-1':             'Stat. char. of lvl 1 pers. silh.',\n",
        "        'silhouette-2':             'Stat. char. of lvl 2 pers. silh.',\n",
        "        \n",
        "        'entropy':                  'Persistence entropy',\n",
        "        'numberofpoints':           'Number of points',\n",
        "    }[row]\n",
        "\n",
        "def DimensionRenamer(row):\n",
        "    return {\n",
        "        'all':        'All',\n",
        "        'dim-1':      'Dimension 1',\n",
        "        'dim-2':      'Dimension 2',\n",
        "        'dim-3':      'Dimension 3',\n",
        "        'dim-4':      'Dimension 4',\n",
        "        'dim-5':      'Dimension 5',\n",
        "        'norm-1':     'Amplitude norm with p=1',\n",
        "        'norm-2':     'Amplitude norm with p=2',\n",
        "    }[row]\n",
        "\n",
        "def StatRenamer(row):\n",
        "    return {\n",
        "        'kurtosis':      'Kurtosis',\n",
        "        'skew':          'Skew',\n",
        "        'max':           'Maximum',\n",
        "        'mean':          'Mean',\n",
        "        'median':        'Median',\n",
        "        'norm-1':        'Norm with p=1',\n",
        "        'norm-2':        'Norm with p=2',\n",
        "        'percentile-25': 'Percentile - 25',\n",
        "        'percentile-75': 'Percentile - 75',\n",
        "        'std':           'Standard deviation',\n",
        "        'sum':           'Sum',\n",
        "    }[row]\n",
        "\n",
        "def make_stats(data: pandas.DataFrame, group_by: str, sort_by: str):\n",
        "    stats = data.groupby(by = group_by).mean(numeric_only = True)\n",
        "    stats = stats.sort_values(by = sort_by).reset_index()\n",
        "    stats.to_csv(f\"{subj}/{exp}/IV/stats_{group_by}_{sort_by}.csv\", index = False)\n",
        "    return stats\n",
        "\n",
        "def draw(\n",
        "    data: pandas.DataFrame,\n",
        "    group_by: str,\n",
        "    sort_by: str,\n",
        "    ax,\n",
        "    renamer: typing.Callable[[str], str],\n",
        "    title: str\n",
        "):\n",
        "    stats = make_stats(data, group_by, sort_by)\n",
        "    stats[group_by] = stats[group_by].apply(renamer)\n",
        "    stats.plot.barh(x = group_by, y = sort_by, ax = ax, xlim = (2 * stats[sort_by].min() / 3, None))\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.get_legend().remove()\n",
        "    ax.xaxis.set_label_text('')\n",
        "    ax.yaxis.set_label_text('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize = (20, 15))\n",
        "fig.suptitle('Mean information value by types of features', fontsize = 16)\n",
        "\n",
        "draw(iv_qsda_scores, 'Source', 'IV', axes.flat[0], SourceRenamer, 'By feature source')\n",
        "draw(iv_qsda_scores, 'Algorithm', 'IV', axes.flat[1], AlgorithmRenamer, 'By extraction method')\n",
        "draw(iv_qsda_scores, 'Dimension', 'IV', axes.flat[2], DimensionRenamer, 'By dimension')\n",
        "draw(iv_qsda_scores, 'Stat', 'IV', axes.flat[3], StatRenamer, 'By statistical characteristic')\n",
        "\n",
        "fig.tight_layout(h_pad = 3)\n",
        "fig.savefig(f\"{subj}/{exp}/IV/feature_agg_by_iv.svg\")\n",
        "fig.savefig(f\"{subj}/{exp}/IV/feature_agg_by_iv.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize = (20, 15))\n",
        "fig.suptitle('Mean QSDA scores by types of features', fontsize = 16)\n",
        "\n",
        "draw(iv_qsda_scores, 'Source', 'score', axes.flat[0], SourceRenamer, 'By feature source')\n",
        "draw(iv_qsda_scores, 'Algorithm', 'score', axes.flat[1], AlgorithmRenamer, 'By extraction method')\n",
        "draw(iv_qsda_scores, 'Dimension', 'score', axes.flat[2], DimensionRenamer, 'By dimension')\n",
        "draw(iv_qsda_scores, 'Stat', 'score', axes.flat[3], StatRenamer, 'By statistical characteristic')\n",
        "\n",
        "fig.tight_layout(h_pad = 3)\n",
        "fig.savefig(f\"{subj}/{exp}/IV/feature_agg_by_qsda.svg\")\n",
        "fig.savefig(f\"{subj}/{exp}/IV/feature_agg_by_qsda.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_sources_heatmap(data: pandas.DataFrame, sort_by: str):\n",
        "    stats = make_stats(data, \"Source\", sort_by)[[\"Source\", sort_by]]\n",
        "    stats = stats[stats.apply(lambda row: row['Source'].startswith('channel-'), axis = 1)]\n",
        "\n",
        "    scores = numpy.zeros(38)\n",
        "    for _, row in stats.iterrows():\n",
        "        id = int(row['Source'][row['Source'].find('-') + 1:])\n",
        "        scores[id] = row[sort_by]\n",
        "\n",
        "    svg = SDA.analytics.draw_sources_heatmap(scores)\n",
        "    file = f\"{subj}/{exp}/IV/{sort_by}_regions.svg\"\n",
        "    open(file, \"w\").write(svg)\n",
        "    return IPython.display.SVG(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "draw_sources_heatmap(iv_qsda_scores, \"IV\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "draw_sources_heatmap(iv_qsda_scores, \"score\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
