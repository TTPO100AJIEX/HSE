{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 1,
    "id": "kr9vAeEQlRVG"
   },
   "source": [
    "# Домашнее задание 2. Классификация изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вступление"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 3,
    "id": "BxX49gLclRVJ"
   },
   "source": [
    "В этом задании потребуется обучить классификатор изображений. Будем работать с датасетом, название которого раскрывать не будем. Можете посмотреть самостоятельно на картинки, которые в есть датасете. В нём 200 классов и около 5 тысяч картинок на каждый класс. Классы пронумерованы, как нетрудно догадаться, от 0 до 199. Скачать датасет можно вот [тут](https://yadi.sk/d/BNR41Vu3y0c7qA).\n",
    "\n",
    "Структура датасета простая -- есть директории train/ и val/, в которых лежат обучающие и валидационные данные. В train/ и val/ лежат директориии, соответствующие классам изображений, в которых лежат, собственно, сами изображения.\n",
    " \n",
    "__Задание__. Необходимо выполнить два задания\n",
    "\n",
    "1) Добейтесь accuracy **на валидации не менее 0.44**. В этом задании **запрещено** пользоваться предобученными моделями и ресайзом картинок. 5 баллов\n",
    "\n",
    "2) Добейтесь accuracy **на валидации не менее 0.84**. В этом задании делать ресайз и использовать претрейн можно. 5 баллов\n",
    "\n",
    "Напишите краткий отчёт о проделанных экспериментах. Что сработало и что не сработало? Почему вы решили, сделать так, а не иначе? Обязательно указывайте ссылки на чужой код, если вы его используете. Обязательно ссылайтесь на статьи / блогпосты / вопросы на stackoverflow / видосы от ютуберов-машинлернеров / курсы / подсказки от Дяди Васи и прочие дополнительные материалы, если вы их используете. \n",
    "\n",
    "Ваш код обязательно должен проходить все `assert`'ы ниже.\n",
    "\n",
    "__Использовать внешние данные для обучения строго запрещено в обоих заданиях. Также запрещено обучаться на валидационной выборке__.\n",
    "\n",
    "\n",
    "__Критерии оценки__: Оценка вычисляется по простой формуле: `min(10, 10 * Ваша accuracy / 0.44)` для первого задания и `min(10, 10 * (Ваша accuracy - 0.5) / 0.34)` для второго. Оценка округляется до десятых по арифметическим правилам.\n",
    "\n",
    "\n",
    "__Советы и указания__:\n",
    " - Наверняка вам потребуется много гуглить о классификации и о том, как заставить её работать. Это нормально, все гуглят. Но не забывайте, что нужно быть готовым за скатанный код отвечать :)\n",
    " - Используйте аугментации. Для этого пользуйтесь модулем `torchvision.transforms` или библиотекой [albumentations](https://github.com/albumentations-team/albumentations)\n",
    " - Можно обучать с нуля или файнтюнить (в зависимости от задания) модели из `torchvision`.\n",
    " - Рекомендуем написать вам сначала класс-датасет (или воспользоваться классом `ImageFolder`), который возвращает картинки и соответствующие им классы, а затем функции для трейна по шаблонам ниже. Однако делать это мы не заставляем. Если вам так неудобно, то можете писать код в удобном стиле. Однако учтите, что чрезмерное изменение нижеперечисленных шаблонов увеличит количество вопросов к вашему коду и повысит вероятность вызова на защиту :)\n",
    " - Валидируйте. Трекайте ошибки как можно раньше, чтобы не тратить время впустую.\n",
    " - Чтобы быстро отладить код, пробуйте обучаться на маленькой части датасета (скажем, 5-10 картинок просто чтобы убедиться что код запускается). Когда вы поняли, что смогли всё отдебажить, переходите обучению по всему датасету\n",
    " - На каждый запуск делайте ровно одно изменение в модели/аугментации/оптимайзере, чтобы понять, что и как влияет на результат.\n",
    " - Фиксируйте random seed.\n",
    " - Начинайте с простых моделей и постепенно переходите к сложным. Обучение лёгких моделей экономит много времени.\n",
    " - Ставьте расписание на learning rate. Уменьшайте его, когда лосс на валидации перестаёт убывать.\n",
    " - Советуем использовать GPU. Если у вас его нет, используйте google colab. Если вам неудобно его использовать на постоянной основе, напишите и отладьте весь код локально на CPU, а затем запустите уже написанный ноутбук в колабе. Авторское решение задания достигает требуемой точности в колабе за 15 минут обучения.\n",
    " \n",
    "Good luck & have fun! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import abc\n",
    "import time\n",
    "import typing\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import PIL\n",
    "import tqdm\n",
    "import torch\n",
    "import wandb\n",
    "import numpy\n",
    "import pandas\n",
    "import torchscan\n",
    "import torchvision\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as torchdata\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(device)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    numpy.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "def fix_random():\n",
    "    return set_random_seed(RANDOM_STATE)\n",
    "fix_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(anonymous = \"allow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**✨ Внимание ✨**\n",
    "\n",
    "В этом домашнем задании предлагается использовать библиотеку `pytorch_lightning`. Доступ к ее [документации](https://lightning.ai/docs/pytorch/stable/) заблокирован с территории РФ. Вы можете:\n",
    "\n",
    "1. Получить к ней доступ с помощью VPN.\n",
    "\n",
    "2. Собрать документацию самостоятельно. Для этого склонируйте [github-репозиторий](https://github.com/Lightning-AI/lightning/tree/master), запустите в нем терминал (на windows – git bash) и выполните команды:\n",
    "\n",
    "```shell\n",
    "git submodule update --init --recursive\n",
    "make docs\n",
    "```\n",
    "После этого откройте появившийся файл `docs/build/html/index.html`. Для работы команд в вашем окружении должен быть `pip`. Полная инструкция [по ссылке](https://github.com/Lightning-AI/lightning/tree/master/docs).\n",
    "\n",
    "3. Гуглить `<error message> pytorch lightning` или `<how to do this> pytorch lightning`. Stack overflow на территории РФ все еще доступен 😉\n",
    "\n",
    "4. Не пользоваться `pytorch_lightning` и написать цикл обучения модели самостоятельно. Например, по аналогии с функцией `fit` из [семинара 4](https://github.com/hse-ds/iad-deep-learning/blob/master/2023/seminars/04.%20Optim%20%26%20Lightning/04_Optim%26Lightning_solution.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RytEDW0ylRVN"
   },
   "source": [
    "## Задание 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HZECedTvepi"
   },
   "source": [
    "### Что поможет сделать на 10 из 10 (одно задание - 5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOioHGEiveso"
   },
   "source": [
    "1. Использовать все возможные методы оптимизации и эксперемнтировать с ними.\n",
    "2. Подбор learning rate. Пример из прошлого семинара как это делать: [Как найти lr](https://pytorch-lightning.readthedocs.io/en/1.4.5/advanced/lr_finder.html)\n",
    "\n",
    "```\n",
    "  trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=2, auto_lr_find=True) \n",
    "\n",
    "  trainer.tune(module, train_dataloader, eval_dataloader)\n",
    "\n",
    "  trainer.fit(module, train_dataloader, eval_dataloader))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "3. Аугментация данных. [Документация (полезная)](https://pytorch.org/vision/main/transforms.html), а также [библиотека albumentation](https://towardsdatascience.com/getting-started-with-albumentation-winning-deep-learning-image-augmentation-technique-in-pytorch-47aaba0ee3f8)\n",
    "4. Подбор архитектуры модели. \n",
    "5. Можно написать модель руками свою в YourNet, а можно импортировать не предобученную сетку известной архитектуры из модуля torchvision.models. Один из способов как можно сделать: \n",
    "\n",
    "  * `torchvision.models.resnet18(pretrained=False, num_classes=200).to(device)`\n",
    "  * Документация по возможным моделям и как их можно брать: [Документация (полезная)](https://pytorch.org/vision/stable/models.html)\n",
    "6. Правильно нормализовывать данные при создании, пример [тык, но тут и в целом гайд от и до](https://www.pluralsight.com/guides/image-classification-with-pytorch)\n",
    "7. Model Checkpointing. Сохраняйте свой прогресс (модели), чтобы когда что-то пойдет не так вы сможете начать с этого места или просто воспроизвести свои результаты модели, которые обучали. \n",
    " * Пример как можно с wandb тут: [Сохраняем лучшие модели в wandb](https://docs.wandb.ai/guides/integrations/lightning)\n",
    " * По простому можно так: [Сохраняем модели в pytorch дока](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYePsQgNRB-n"
   },
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MysteriousDataset(torchdata.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            train: bool,\n",
    "            preload: bool = True,\n",
    "            precalculate_transform: bool = True,\n",
    "            transform: typing.Optional[transforms.Compose] = None\n",
    "        ):\n",
    "        # Create a way to easily read from disk\n",
    "        self.name = \"train\" if train else \"val\"\n",
    "        self.dataset_src = \"./dataset/{}\".format(self.name)\n",
    "        self.dataset = torchvision.datasets.ImageFolder(self.dataset_src)\n",
    "\n",
    "        self.classes = self.dataset.classes\n",
    "        self.precalculated_transform = None\n",
    "        self.transform = None\n",
    "\n",
    "        # The dataset is not that big, so we might want to load it into RAM beforehand\n",
    "        if preload or precalculate_transform:\n",
    "            if precalculate_transform:\n",
    "                # We might also want to precalculate the transform\n",
    "                self.precalculated_transform = transform\n",
    "                self.transform = transform\n",
    "                transform = None\n",
    "            # If precalculate_transform is True, self.transform is set, and __getitem__ will apply the transforms\n",
    "            self.images, self.targets = self.load_all(\"Preload {}\".format(self.name))\n",
    "        # If precalculate_transform is True, this will be None\n",
    "        self.transform = transform\n",
    "\n",
    "    def load_all(self, progress_bar: bool = False):\n",
    "        images = [ ]\n",
    "        targets = [ ]\n",
    "        for record in (tqdm.tqdm(self, desc = progress_bar) if progress_bar else self):\n",
    "            images.append(record[0])\n",
    "            targets.append(record[1])\n",
    "        # If no transforms are applied, torch.stack(images) will fail\n",
    "        try: return torch.stack(images), targets\n",
    "        except: return images, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if hasattr(self, 'images') and hasattr(self, 'targets'):\n",
    "            # If the data has been preloaded, use it\n",
    "            image, target = self.images[idx], self.targets[idx]\n",
    "        else:\n",
    "            # Access the source\n",
    "            image, target = self.dataset[idx]\n",
    "        # Apply the transform if needed\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, target\n",
    "    \n",
    "    def channel_stats(self):\n",
    "        images, _ = self.load_all() # Get the dataset as two tensors\n",
    "        # Calculate the metrics\n",
    "        mean = torch.mean(images, dim = [0, 2, 3])\n",
    "        std = torch.std(images, dim = [0, 2, 3])\n",
    "        return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.v2.Compose([\n",
    "    torchvision.transforms.v2.ToImage(),\n",
    "    torchvision.transforms.v2.ToDtype(torch.float32, scale = True)\n",
    "])\n",
    "\n",
    "no_preload = MysteriousDataset(True, transform = transform, preload = False, precalculate_transform = False)\n",
    "no_precalc = MysteriousDataset(True, transform = transform, preload = True, precalculate_transform = False)\n",
    "preload = MysteriousDataset(True, transform = transform, preload = True, precalculate_transform = True)\n",
    "torch_dataset = torchvision.datasets.ImageFolder(\"./dataset/train\", transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that it is actually faster to loop through the dataset when it is preloaded into RAM\n",
    "def test_loading(*args):\n",
    "    for dataset, desc in args:\n",
    "        loader = torchdata.DataLoader(dataset, batch_size = 256)\n",
    "        for item in tqdm.tqdm(loader, desc = desc):\n",
    "            pass\n",
    "\n",
    "test_loading(\n",
    "    (torch_dataset, 'ImageFolder'),\n",
    "    (no_preload, 'no_preload'),\n",
    "    (no_precalc, 'no_precalc'),\n",
    "    (preload, 'preload')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посчитаем поканальные средние и стандартные отклонения для нормализации данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = preload.channel_stats()\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del torch_dataset\n",
    "del no_preload\n",
    "del no_precalc\n",
    "del preload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создадим датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "\n",
    "# Transforms\n",
    "transform = torchvision.transforms.v2.Compose([\n",
    "    torchvision.transforms.v2.ToImage(),\n",
    "    torchvision.transforms.v2.ToDtype(torch.float32, scale = True),\n",
    "    torchvision.transforms.v2.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_set = MysteriousDataset(train = True, preload = True, precalculate_transform = True, transform = transform)\n",
    "test_set = MysteriousDataset(train = False, preload = True, precalculate_transform = True, transform = transform)\n",
    "\n",
    "# Check\n",
    "print(len(train_set), len(test_set))\n",
    "\n",
    "# Just very simple sanity checks\n",
    "assert isinstance(train_set[0], tuple)\n",
    "assert len(train_set[0]) == 2\n",
    "assert isinstance(train_set[1][1], int)\n",
    "assert isinstance(train_set[1][0], torch.Tensor)\n",
    "assert train_set[1][0].shape == torch.Size([ 3, 64, 64 ])\n",
    "print(\"Dataset tests passed\")\n",
    "\n",
    "for images, targets in torchdata.DataLoader(train_set, batch_size = 256):\n",
    "    assert isinstance(images, torch.Tensor)\n",
    "    assert isinstance(targets, torch.Tensor)\n",
    "    assert images.shape == torch.Size([ 256, 3, 64, 64 ])\n",
    "    assert targets.shape == torch.Size([ 256 ])\n",
    "    print(\"DataLoader tests passed\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOuM0EEYj7Ml"
   },
   "source": [
    "### Посмотрим на картиночки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "\n",
    "# Denormalization\n",
    "denormalize = torchvision.transforms.v2.Compose([\n",
    "    torchvision.transforms.v2.Normalize(mean = [ 0., 0., 0. ], std = 1 / std),\n",
    "    torchvision.transforms.v2.Normalize(mean = -mean, std = [ 1., 1., 1. ])\n",
    "])\n",
    "\n",
    "# Display some samples from each dataset\n",
    "def display_examples(dataset: MysteriousDataset, row: int):\n",
    "    train_loader = torchdata.DataLoader(dataset, batch_size = 10, shuffle = True)\n",
    "    for i, (image, label) in enumerate(zip(*next(iter(train_loader)))):\n",
    "        plt.subplot(3, 10, i + 10 * (row - 1) + 1)\n",
    "        plt.axis('off')\n",
    "        plt.title('{}'.format(label))\n",
    "        plt.imshow((denormalize(image).permute(1, 2, 0).numpy() * 255).astype(numpy.uint8))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 5)\n",
    "display_examples(train_set, 1)\n",
    "display_examples(test_set, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCwKB-3nKm1-"
   },
   "source": [
    "## Задание 1. \n",
    "\n",
    "5 баллов\n",
    "Добейтесь accuracy на валидации не менее 0.44. В этом задании запрещено пользоваться предобученными моделями и ресайзом картинок.\n",
    "\n",
    "\n",
    "Для того чтобы выбить скор (считается ниже) на 2.5/5 балла (то есть половину за задание) достаточно соблюдать пару простых жизненных правил:\n",
    "1. Аугментация (без нее сложно очень будет)\n",
    "2. Оптимайзеры можно (и нужно) использовать друг с другом. Однако когда что-то проверяете, то не меняйте несколько параметров сразу - собьете логику экспериментов\n",
    "3. Не используйте полносвязные модели или самые первые сверточные, используйте более современные архитектуры (что на лекциях встречались)\n",
    "4. Посмотреть все ноутбуки прошедших семинаров и слепить из них что-то общее. Семинарских тетрадок хватит сверх"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseClassifier(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, train_set: torchdata.Dataset, val_set: torchdata.Dataset):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def predict(self, images: torch.Tensor) -> typing.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def calc_metrics(self, dataset: torchdata.Dataset) -> dict:\n",
    "        num_classes = self.output_shape[0] if hasattr(self, 'output_shape') else len(dataset.classes)\n",
    "        classes = torch.arange(num_classes)\n",
    "\n",
    "        all_labels = torch.tensor([])\n",
    "        all_predictions = torch.tensor([])\n",
    "        all_scores = torch.empty((0, num_classes))\n",
    "        loader = torchdata.DataLoader(dataset, batch_size = 512, shuffle = False)\n",
    "        for images, labels in loader:\n",
    "            predictions, scores = self.predict(images)\n",
    "            all_labels = torch.cat([ all_labels, labels ])\n",
    "            all_scores = torch.cat([ all_scores, scores.detach().cpu() ])\n",
    "            all_predictions = torch.cat([ all_predictions, predictions.detach().cpu() ])\n",
    "\n",
    "        return {\n",
    "            'Accuracy':       sklearn.metrics.accuracy_score      (all_labels, all_predictions),\n",
    "            'TOP-2 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 2, labels = classes),\n",
    "            'TOP-3 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 3, labels = classes),\n",
    "            'TOP-4 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 4, labels = classes),\n",
    "            'TOP-5 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 5, labels = classes),\n",
    "            'TOP-6 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 6, labels = classes),\n",
    "            'TOP-7 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 7, labels = classes),\n",
    "            'TOP-8 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 8, labels = classes),\n",
    "            'TOP-9 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 9, labels = classes),\n",
    "            # 'AUC-ROC':        sklearn.metrics.roc_auc_score       (all_labels, all_scores, multi_class = 'ovo'),\n",
    "            'Precision':      sklearn.metrics.precision_score     (all_labels, all_predictions, average = 'macro', zero_division = 0),\n",
    "            'Recall':         sklearn.metrics.recall_score        (all_labels, all_predictions, average = 'macro', zero_division = 0),\n",
    "            'F1-score':       sklearn.metrics.f1_score            (all_labels, all_predictions, average = 'macro', zero_division = 0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(BaseClassifier):\n",
    "    results = [ ]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            name: str,\n",
    "            model: torch.nn.Module,\n",
    "            batch_size: int = 256,\n",
    "            learning_rate: int = 1e-3,\n",
    "            device: torch.device = device,\n",
    "            optimizer: typing.Optional[torch.optim.Optimizer] = None,\n",
    "            scheduler: typing.Optional[torch.optim.lr_scheduler.LRScheduler] = None,\n",
    "        ):\n",
    "        self.name = name\n",
    "        self.history = [ ]\n",
    "        self.device = device\n",
    "        self.input_shape = None\n",
    "        self.output_shape = None\n",
    "        self.scheduler = scheduler\n",
    "        self.batch_size = batch_size\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = optimizer or torch.optim.AdamW(self.model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "    def train(self, images: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "        self.model.train() # Enter train mode\n",
    "        self.optimizer.zero_grad() # Zero gradients\n",
    "        output = self.model(images.to(self.device)) # Get predictions\n",
    "        loss = torch.nn.functional.cross_entropy(output, labels.to(self.device)) # Calculate loss\n",
    "        loss.backward() # Calculate gradients\n",
    "        self.optimizer.step() # Update weights\n",
    "        return loss.item()\n",
    "\n",
    "    def train_epoch(self, loader: torchdata.DataLoader) -> float:\n",
    "        sum_loss = 0\n",
    "        for images, labels in loader:\n",
    "            sum_loss += self.train(images, labels) # Train one batch\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step()\n",
    "        return sum_loss / len(loader) # Return average loss to avoid random-dependent graph\n",
    "       \n",
    "    def fit(self, train_set: torchdata.Dataset, val_set: torchdata.Dataset, n_epochs: int = 25, silent: bool = False):\n",
    "        if self.input_shape is None or self.output_shape is None:\n",
    "            self.predict(train_set[0][0].unsqueeze(0)) # Initialize lazy layers and in/out shapes\n",
    "        loader = torchdata.DataLoader(train_set, batch_size = self.batch_size, shuffle = True)\n",
    "\n",
    "        wandb_settings = { \"silent\": True, \"disable_git\": True } if silent else None\n",
    "        wandb.init(project = \"DL-HW-2\", name = self.name, anonymous = \"allow\", settings = wandb_settings)\n",
    "        wandb.watch(self.model, log = \"all\")\n",
    "\n",
    "        for epoch in tqdm.trange(n_epochs, desc = \"Fit {}\".format(self.name)):\n",
    "            # Train\n",
    "            train_start = time.perf_counter()\n",
    "            loss = self.train_epoch(loader)\n",
    "            train_time = time.perf_counter() - train_start\n",
    "\n",
    "            # Validate\n",
    "            val_start = time.perf_counter()\n",
    "            metrics = self.calc_metrics(val_set)\n",
    "            val_time = time.perf_counter() - val_start\n",
    "            \n",
    "            # Upload metrics\n",
    "            metrics['Validation time'] = val_time\n",
    "            metrics['Train time'] = train_time\n",
    "            metrics['Loss'] = loss\n",
    "            wandb.log(metrics)\n",
    "            metrics['Epoch'] = epoch + 1\n",
    "            self.history.append(metrics)\n",
    "\n",
    "        # Finish the run\n",
    "        wandb.finish(quiet = True)\n",
    "\n",
    "        # Store best metrics\n",
    "        self.best_metrics = max(self.history, key = lambda item: item['Accuracy'])\n",
    "        Classifier.results.append({ 'Name': self.name, **self.best_metrics })\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def predict(self, images: torch.Tensor) -> typing.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if self.input_shape is None:\n",
    "            self.input_shape = images[0].shape # Lazily initialize input shape\n",
    "\n",
    "        self.model.eval() # Enter evaluation mode\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(images.to(self.device)) # Get outputs\n",
    "            scores = torch.softmax(outputs, dim = 1) # Make probabilities\n",
    "            predictions = torch.argmax(scores, dim = 1) # Calculate predictions\n",
    "\n",
    "        if self.output_shape is None:\n",
    "            self.output_shape = scores[0].shape # Lazily initialize output shape\n",
    "        return predictions, scores\n",
    "    \n",
    "\n",
    "    def summary(self):\n",
    "        display(pandas.DataFrame(Classifier.results)) # Print run history\n",
    "        warnings.filterwarnings(\"ignore\") # Ignore warnings that might be printed by torchscan.summary\n",
    "        torchscan.summary(self.model.eval(), self.input_shape, receptive_field = True) # Print summary about this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать свертки размера 3x3 с отступом 1. Количество фильтров будем увеличивать в два раза с каждой сверткой, начиная с 16. После каждого слоя будем применять нормализацию и макс-пулинг размера 2x2. \\\n",
    "Для собственно классификации создадим два полносвязных слоя с 1024 и 200 нейронами соответственно (выходы второго - ответы модели для каждого класса). Между слоями добавим нормализацию, а во избежание переобучения перед обоими слоями будем использовать Dropout с вероятностью 0.5. \\\n",
    "В качестве нелинейности выберем GELU. В прошлом ДЗ эта функция показала себя немного лучше других, но сильного влияния на качество это не должно оказывать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем модели с различным количеством сверточных слоев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(16), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(32), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(64), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "\n",
    "    torch.nn.Flatten(), torch.nn.Dropout(0.5), torch.nn.LazyLinear(1024), torch.nn.BatchNorm1d(1024), torch.nn.GELU(),\n",
    "    torch.nn.Dropout(0.5), torch.nn.Linear(1024, 200)\n",
    ")\n",
    "Classifier('3 convolutions', model).fit(train_set, test_set).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(16), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(32), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(64), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(128), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "\n",
    "    torch.nn.Flatten(), torch.nn.Dropout(0.5), torch.nn.LazyLinear(1024), torch.nn.BatchNorm1d(1024), torch.nn.GELU(),\n",
    "    torch.nn.Dropout(0.5), torch.nn.Linear(1024, 200)\n",
    ")\n",
    "Classifier('4 convolutions', model).fit(train_set, test_set).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(16), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(32), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(64), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(128), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(256), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "\n",
    "    torch.nn.Flatten(), torch.nn.Dropout(0.5), torch.nn.LazyLinear(1024), torch.nn.BatchNorm1d(1024), torch.nn.GELU(),\n",
    "    torch.nn.Dropout(0.5), torch.nn.Linear(1024, 200)\n",
    ")\n",
    "Classifier('5 convolutions', model).fit(train_set, test_set).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавление четвертого слоя заметно улучшило качество модели, но пятый слой лишь немного его понизил. \\\n",
    "Заметим, что поле восприятия пятого слоя - 94 - явно больше размера обрабатываемых картинок (64x64), а третьего слоя - 22 - заметно меньше. Четвёртый слой же \"видит\" 46 пикселей, что наиболее близко к размеру картинки. Следовательно, можно предоложить, что наилучшего качества достигнет модель с полем восприятия, равным размеру картинки (не больше и не меньше)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем убрать одно применения пулинга после четвертого слоя. Это понизит поле восприятия до 62."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(16), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(32), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(64), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(128), torch.nn.GELU(),\n",
    "    torch.nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(256), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "\n",
    "    torch.nn.Flatten(), torch.nn.Dropout(0.5), torch.nn.LazyLinear(1024), torch.nn.BatchNorm1d(1024), torch.nn.GELU(),\n",
    "    torch.nn.Dropout(0.5), torch.nn.Linear(1024, 200)\n",
    ")\n",
    "Classifier('Receptive Field 62', model).fit(train_set, test_set).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, эта модель показывает гораздо более хорошее качество. Попробуем увеличить поле восприятия до 64 добавлением ещё одного свёрточного в начале."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(16), torch.nn.GELU(),\n",
    "    torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(32), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(64), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(128), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(256), torch.nn.GELU(),\n",
    "    torch.nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(512), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "\n",
    "    torch.nn.Flatten(), torch.nn.Dropout(0.5), torch.nn.LazyLinear(1024), torch.nn.BatchNorm1d(1024), torch.nn.GELU(),\n",
    "    torch.nn.Dropout(0.5), torch.nn.Linear(1024, 200)\n",
    ")\n",
    "Classifier('Receptive Field 64', model).fit(train_set, test_set).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стало однозначно лучше. Попробуем добавить scheduler, чтобы стабилизировать процесс обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(16), torch.nn.GELU(),\n",
    "    torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(32), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(64), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(128), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(256), torch.nn.GELU(),\n",
    "    torch.nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(512), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "\n",
    "    torch.nn.Flatten(), torch.nn.Dropout(0.5), torch.nn.LazyLinear(1024), torch.nn.BatchNorm1d(1024), torch.nn.GELU(),\n",
    "    torch.nn.Dropout(0.5), torch.nn.Linear(1024, 200)\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9)\n",
    "Classifier('Scheduler', model, optimizer = optimizer, scheduler = scheduler).fit(train_set, test_set).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшее качество достигается на 22 эпохе. Возьмём эту модель в качестве итоговой. Обучим заново для валидации результата (к счастью, это занимает не очень много времени)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итоговая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(16), torch.nn.GELU(),\n",
    "    torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(32), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(64), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(128), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(256), torch.nn.GELU(),\n",
    "    torch.nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(512), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "\n",
    "    torch.nn.Flatten(), torch.nn.Dropout(0.5), torch.nn.LazyLinear(1024), torch.nn.BatchNorm1d(1024), torch.nn.GELU(),\n",
    "    torch.nn.Dropout(0.5), torch.nn.Linear(1024, 200)\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9)\n",
    "final_model = Classifier('Task 1 final', model, optimizer = optimizer, scheduler = scheduler).fit(train_set, test_set, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = final_model.calc_metrics(test_set)['Accuracy']\n",
    "print(f\"Accuracy: {accuracy * 100}%\")\n",
    "print(f\"Оценка за это задание составит {numpy.clip(10 * accuracy / 0.44, 0, 10):.2f} баллов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZqSdlQQKukS"
   },
   "source": [
    "## Задание 2\n",
    "\n",
    "5 баллов\n",
    "Добейтесь accuracy на валидации не менее 0.84. В этом задании делать ресайз и использовать претрейн можно.\n",
    "\n",
    "Для того чтобы выбить скор (считается ниже) на 2.5/5 балла (то есть половину за задание) достаточно соблюдать пару простых жизненных правил:\n",
    "1. Аугментация (без нее сложно очень будет)\n",
    "2. Оптимайзеры можно (и нужно) использовать друг с другом. Однако когда что-то проверяете, то не меняйте несколько параметров сразу - собьете логику экспериментов\n",
    "3. Не используйте полносвязные модели или самые первые сверточные, используйте более современные архитектуры (что на лекциях встречались или можете пойти дальше).\n",
    "4. Попробуйте сначала посмотреть качество исходной модели без дообучения, сохраните как baseline. Отсюда поймете какие слои нужно дообучать.\n",
    "5. Посмотреть все ноутбуки прошедших семинаров и слепить из них что-то общее. Семинарских тетрадок хватит сверх"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что нужное качество можно достичь без аугментаций, поэтому будем просто применять предобученную модель ко всем картинкам до обучения с сохранением результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesDataset(torchdata.StackDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            extractor_name: str,\n",
    "            dataset: torchdata.Dataset,\n",
    "            batch_size: int = 256,\n",
    "            extractor_device: torch.device = device\n",
    "        ):\n",
    "        self.name = 'Features for {}'.format(dataset.name if hasattr(dataset, 'name') else 'undefined')\n",
    "\n",
    "        # If it is already a dataset of features, return\n",
    "        if isinstance(dataset, FeaturesDataset):\n",
    "            return super().__init__(dataset)\n",
    "\n",
    "        # https://github.com/pytorch/vision/issues/7744\n",
    "        def get_state_dict(self, *args, **kwargs):\n",
    "            kwargs.pop(\"check_hash\")\n",
    "            return torch.hub.load_state_dict_from_url(self.url, *args, **kwargs)\n",
    "        torchvision.models._api.WeightsEnum.get_state_dict = get_state_dict\n",
    "\n",
    "        # Load a pretrained model\n",
    "        self.weights = torchvision.models.get_model_weights(extractor_name).DEFAULT\n",
    "        self.extractor = torchvision.models.get_model(extractor_name, weights = self.weights)\n",
    "        self.transform = self.weights.transforms()\n",
    "        self.extractor_device = extractor_device\n",
    "\n",
    "        # Remove last layer\n",
    "        if hasattr(self.extractor, 'fc'): self.extractor.fc = torch.nn.Identity() # ResNet\n",
    "        elif hasattr(self.extractor, 'classifier'): self.extractor.classifier = torch.nn.Identity() # EfficientNet\n",
    "        \n",
    "        save_transform = dataset.transform if hasattr(dataset, 'transform') else None\n",
    "        dataset.transform = None\n",
    "        assert isinstance(dataset[0][0], PIL.Image.Image) # Without transforms it should return raw images\n",
    "        dataset.transform = self.transform # Use transforms for pretrained model\n",
    "\n",
    "        targets = [ ]\n",
    "        features = [ ]\n",
    "        self.extractor.to(self.extractor_device).eval() # Enter evaluation mode\n",
    "        loader = torchdata.DataLoader(dataset, batch_size = batch_size) # Create a dataloader\n",
    "        for images_batch, targets_batch in tqdm.tqdm(loader, desc = self.name):\n",
    "            with torch.no_grad():\n",
    "                # Calculate features\n",
    "                features_batch = self.extractor(images_batch.to(self.extractor_device))\n",
    "                features.append(features_batch.detach().cpu().flatten(start_dim = 1))\n",
    "                targets.append(targets_batch)\n",
    "\n",
    "        # Free up the GPU\n",
    "        self.extractor = self.extractor.to('cpu')\n",
    "        if self.extractor_device.type == 'cuda': torch.cuda.empty_cache()\n",
    "        elif self.extractor_device.type == 'mps': torch.mps.empty_cache()\n",
    "\n",
    "        dataset.transform = save_transform # Restore transforms of the base dataset\n",
    "        super().__init__(torch.cat(features), torch.cat(targets)) # Initialize StackDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make datasets without transforms to calculate image features\n",
    "train_set = MysteriousDataset(train = True, preload = True, precalculate_transform = True, transform = None)\n",
    "test_set = MysteriousDataset(train = False, preload = True, precalculate_transform = True, transform = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем различные модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"alexnet\", 1024),\n",
    "    (\"convnext_base\", 128),\n",
    "    (\"densenet201\", 256),\n",
    "    (\"efficientnet_b4\", 64),\n",
    "    (\"efficientnet_v2_s\", 128),\n",
    "    (\"googlenet\", 512),\n",
    "    (\"inception_v3\", 256),\n",
    "    (\"mnasnet1_3\", 512),\n",
    "    (\"mobilenet_v2\", 256),\n",
    "    (\"mobilenet_v3_large\", 512),\n",
    "    (\"regnet_x_8gf\", 128),\n",
    "    (\"resnet152\", 256),\n",
    "    (\"resnext101_64x4d\", 128),\n",
    "    (\"shufflenet_v2_x2_0\", 512),\n",
    "    (\"vgg19_bn\", 64),\n",
    "    (\"wide_resnet101_2\", 128)\n",
    "]\n",
    "for index, (extractor_name, batch_size) in enumerate(models):\n",
    "    print(\"{} ({}/{}):\".format(extractor_name.upper(), index + 1, len(models)))\n",
    "\n",
    "    fix_random()\n",
    "    train_features = FeaturesDataset(extractor_name, train_set, batch_size = batch_size)\n",
    "    test_features = FeaturesDataset(extractor_name, test_set, batch_size = batch_size)\n",
    "    model = torch.nn.Sequential(torch.nn.LazyLinear(1024), torch.nn.GELU(), torch.nn.Linear(1024, 200))\n",
    "    Classifier(extractor_name, model, learning_rate = 3e-5).fit(train_features, test_features, silent = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что среди рассмотренных моделей resnext101_64x4d показывает наилучшее качество и достигает требуемых 84%. Попробуем немного улучшить параметры модели, чтобы она достигала требуемого качества более уверенно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "train_features = FeaturesDataset(\"resnext101_64x4d\", train_set)\n",
    "test_features = FeaturesDataset(\"resnext101_64x4d\", test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Количество слоев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "model = torch.nn.Sequential(torch.nn.LazyLinear(1024), torch.nn.GELU(), torch.nn.Linear(1024, 200))\n",
    "Classifier('1024 + 200', model, learning_rate = 3e-5).fit(train_features, test_features, 100).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "model = torch.nn.LazyLinear(200)\n",
    "Classifier('200', model, learning_rate = 3e-5).fit(train_features, test_features, 100).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "model = torch.nn.Sequential(torch.nn.LazyLinear(1024), torch.nn.GELU(), torch.nn.Linear(1024, 512), torch.nn.GELU(), torch.nn.Linear(512, 200))\n",
    "Classifier('1024 + 512 + 200', model, learning_rate = 3e-5).fit(train_features, test_features, 100).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наибольшего качества достигает модель с двумя полносвязными слоями с 1024 и 200 нейронами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нормализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "model = torch.nn.Sequential(torch.nn.LazyLinear(1024), torch.nn.BatchNorm1d(1024), torch.nn.GELU(), torch.nn.Linear(1024, 200))\n",
    "Classifier('1024 + BatchNorm + 200', model, learning_rate = 3e-5).fit(train_features, test_features, 100).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавление нормализации ускоряет обучение (модель быстрее доходит до качества, близкого к максимальному), но повышения качества не наблюдается. Тем не менее явно наблюдается переобучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем бороться с переобучением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "model = torch.nn.Sequential(torch.nn.LazyLinear(1024), torch.nn.GELU(), torch.nn.Dropout(0.5), torch.nn.Linear(1024, 200))\n",
    "Classifier('1024 + Dropout + 200', model, learning_rate = 3e-5).fit(train_features, test_features, 100).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стало немного лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем другую функцию активации, которая используется в resnext101_64x4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "model = torch.nn.Sequential(torch.nn.LazyLinear(1024), torch.nn.ReLU(), torch.nn.Dropout(0.5), torch.nn.Linear(1024, 200))\n",
    "Classifier('1024 + Dropout + 200, ReLU', model, learning_rate = 3e-5).fit(train_features, test_features, 100).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стало немного лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итоговая модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшее качество достигается на 93 эпохе. Возьмём эту модель в качестве итоговой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "model = torch.nn.Sequential(torch.nn.LazyLinear(1024), torch.nn.ReLU(), torch.nn.Dropout(0.5), torch.nn.Linear(1024, 200))\n",
    "final_model = Classifier('Task 2 final', model, learning_rate = 3e-5).fit(train_features, test_features, 93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = final_model.calc_metrics(test_features)['Accuracy']\n",
    "print(f\"Accuracy: {accuracy * 100}%\")\n",
    "print(f\"Оценка за это задание составит {numpy.clip(10 * (accuracy - 0.5) / 0.34, 0, 10):.2f} баллов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 15,
    "id": "pT8vfPSolRVb"
   },
   "source": [
    "## Отчёт об экспериментах "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "В ходе экспериментов было выявлено, что наибольшего качества достигает модель, поле восприятия которой равно размеру картинки: в этом случае модель учитывает всю предоставленную информацию (\"видит\" всю картинку, а не ее части), но и не уменьшает картинку слишком сильно (что происходит при слишком большом поле восприятия). С учетом этого была построена достаточно простая модель, состоящая из 6 сверточных слоев с размерами ядер 3x3, между некоторыми из которых происходит применение макс-пулинга размера 2x2. Такая модель верно предсказывает класс для почти 47% объектов (что уверенно превышает требуемое значение 44%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "max_cell_id": 35
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
