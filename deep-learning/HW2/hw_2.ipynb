{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 1,
    "id": "kr9vAeEQlRVG"
   },
   "source": [
    "# –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ 2. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 3,
    "id": "BxX49gLclRVJ"
   },
   "source": [
    "–í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±—É—á–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ë—É–¥–µ–º —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º, –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–≥–æ —Ä–∞—Å–∫—Ä—ã–≤–∞—Ç—å –Ω–µ –±—É–¥–µ–º. –ú–æ–∂–µ—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤ –µ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç–µ. –í –Ω—ë–º 200 –∫–ª–∞—Å—Å–æ–≤ –∏ –æ–∫–æ–ª–æ 5 —Ç—ã—Å—è—á –∫–∞—Ä—Ç–∏–Ω–æ–∫ –Ω–∞ –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Å. –ö–ª–∞—Å—Å—ã –ø—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω—ã, –∫–∞–∫ –Ω–µ—Ç—Ä—É–¥–Ω–æ –¥–æ–≥–∞–¥–∞—Ç—å—Å—è, –æ—Ç 0 –¥–æ 199. –°–∫–∞—á–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –º–æ–∂–Ω–æ –≤–æ—Ç [—Ç—É—Ç](https://yadi.sk/d/BNR41Vu3y0c7qA).\n",
    "\n",
    "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø—Ä–æ—Å—Ç–∞—è -- –µ—Å—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ train/ –∏ val/, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –ª–µ–∂–∞—Ç –æ–±—É—á–∞—é—â–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –í train/ –∏ val/ –ª–µ–∂–∞—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª–∞—Å—Å–∞–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –ª–µ–∂–∞—Ç, —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ, —Å–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n",
    " \n",
    "__–ó–∞–¥–∞–Ω–∏–µ__. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –¥–≤–∞ –∑–∞–¥–∞–Ω–∏—è\n",
    "\n",
    "1) –î–æ–±–µ–π—Ç–µ—Å—å accuracy **–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–µ –º–µ–Ω–µ–µ 0.44**. –í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ **–∑–∞–ø—Ä–µ—â–µ–Ω–æ** –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ —Ä–µ—Å–∞–π–∑–æ–º –∫–∞—Ä—Ç–∏–Ω–æ–∫. 5 –±–∞–ª–ª–æ–≤\n",
    "\n",
    "2) –î–æ–±–µ–π—Ç–µ—Å—å accuracy **–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–µ –º–µ–Ω–µ–µ 0.84**. –í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –¥–µ–ª–∞—Ç—å —Ä–µ—Å–∞–π–∑ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ—Ç—Ä–µ–π–Ω –º–æ–∂–Ω–æ. 5 –±–∞–ª–ª–æ–≤\n",
    "\n",
    "–ù–∞–ø–∏—à–∏—Ç–µ –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á—ë—Ç –æ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö. –ß—Ç–æ —Å—Ä–∞–±–æ—Ç–∞–ª–æ –∏ —á—Ç–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ? –ü–æ—á–µ–º—É –≤—ã —Ä–µ—à–∏–ª–∏, —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, –∞ –Ω–µ –∏–Ω–∞—á–µ? –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑—ã–≤–∞–π—Ç–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —á—É–∂–æ–π –∫–æ–¥, –µ—Å–ª–∏ –≤—ã –µ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å—Å—ã–ª–∞–π—Ç–µ—Å—å –Ω–∞ —Å—Ç–∞—Ç—å–∏ / –±–ª–æ–≥–ø–æ—Å—Ç—ã / –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ stackoverflow / –≤–∏–¥–æ—Å—ã –æ—Ç —é—Ç—É–±–µ—Ä–æ–≤-–º–∞—à–∏–Ω–ª–µ—Ä–Ω–µ—Ä–æ–≤ / –∫—É—Ä—Å—ã / –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ—Ç –î—è–¥–∏ –í–∞—Å–∏ –∏ –ø—Ä–æ—á–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –µ—Å–ª–∏ –≤—ã –∏—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ. \n",
    "\n",
    "–í–∞—à –∫–æ–¥ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–µ–Ω –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –≤—Å–µ `assert`'—ã –Ω–∏–∂–µ.\n",
    "\n",
    "__–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–Ω–µ—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä–æ–≥–æ –∑–∞–ø—Ä–µ—â–µ–Ω–æ –≤ –æ–±–æ–∏—Ö –∑–∞–¥–∞–Ω–∏—è—Ö. –¢–∞–∫–∂–µ –∑–∞–ø—Ä–µ—â–µ–Ω–æ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ__.\n",
    "\n",
    "\n",
    "__–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏__: –û—Ü–µ–Ω–∫–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –ø–æ –ø—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º—É–ª–µ: `min(10, 10 * –í–∞—à–∞ accuracy / 0.44)` –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è –∏ `min(10, 10 * (–í–∞—à–∞ accuracy - 0.5) / 0.34)` –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ. –û—Ü–µ–Ω–∫–∞ –æ–∫—Ä—É–≥–ª—è–µ—Ç—Å—è –¥–æ –¥–µ—Å—è—Ç—ã—Ö –ø–æ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–∞–≤–∏–ª–∞–º.\n",
    "\n",
    "\n",
    "__–°–æ–≤–µ—Ç—ã –∏ —É–∫–∞–∑–∞–Ω–∏—è__:\n",
    " - –ù–∞–≤–µ—Ä–Ω—è–∫–∞ –≤–∞–º –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –º–Ω–æ–≥–æ –≥—É–≥–ª–∏—Ç—å –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –æ —Ç–æ–º, –∫–∞–∫ –∑–∞—Å—Ç–∞–≤–∏—Ç—å –µ—ë —Ä–∞–±–æ—Ç–∞—Ç—å. –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –≤—Å–µ –≥—É–≥–ª—è—Ç. –ù–æ –Ω–µ –∑–∞–±—ã–≤–∞–π—Ç–µ, —á—Ç–æ –Ω—É–∂–Ω–æ –±—ã—Ç—å –≥–æ—Ç–æ–≤—ã–º –∑–∞ —Å–∫–∞—Ç–∞–Ω–Ω—ã–π –∫–æ–¥ –æ—Ç–≤–µ—á–∞—Ç—å :)\n",
    " - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. –î–ª—è —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑—É–π—Ç–µ—Å—å –º–æ–¥—É–ª–µ–º `torchvision.transforms` –∏–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π [albumentations](https://github.com/albumentations-team/albumentations)\n",
    " - –ú–æ–∂–Ω–æ –æ–±—É—á–∞—Ç—å —Å –Ω—É–ª—è –∏–ª–∏ —Ñ–∞–π–Ω—Ç—é–Ω–∏—Ç—å (–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞–Ω–∏—è) –º–æ–¥–µ–ª–∏ –∏–∑ `torchvision`.\n",
    " - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –Ω–∞–ø–∏—Å–∞—Ç—å –≤–∞–º —Å–Ω–∞—á–∞–ª–∞ –∫–ª–∞—Å—Å-–¥–∞—Ç–∞—Å–µ—Ç (–∏–ª–∏ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∫–ª–∞—Å—Å–æ–º `ImageFolder`), –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∏–º –∫–ª–∞—Å—Å—ã, –∞ –∑–∞—Ç–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ç—Ä–µ–π–Ω–∞ –ø–æ —à–∞–±–ª–æ–Ω–∞–º –Ω–∏–∂–µ. –û–¥–Ω–∞–∫–æ –¥–µ–ª–∞—Ç—å —ç—Ç–æ –º—ã –Ω–µ –∑–∞—Å—Ç–∞–≤–ª—è–µ–º. –ï—Å–ª–∏ –≤–∞–º —Ç–∞–∫ –Ω–µ—É–¥–æ–±–Ω–æ, —Ç–æ –º–æ–∂–µ—Ç–µ –ø–∏—Å–∞—Ç—å –∫–æ–¥ –≤ —É–¥–æ–±–Ω–æ–º —Å—Ç–∏–ª–µ. –û–¥–Ω–∞–∫–æ —É—á—Ç–∏—Ç–µ, —á—Ç–æ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –Ω–∏–∂–µ–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤ —É–≤–µ–ª–∏—á–∏—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –∫ –≤–∞—à–µ–º—É –∫–æ–¥—É –∏ –ø–æ–≤—ã—Å–∏—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–∑–æ–≤–∞ –Ω–∞ –∑–∞—â–∏—Ç—É :)\n",
    " - –í–∞–ª–∏–¥–∏—Ä—É–π—Ç–µ. –¢—Ä–µ–∫–∞–π—Ç–µ –æ—à–∏–±–∫–∏ –∫–∞–∫ –º–æ–∂–Ω–æ —Ä–∞–Ω—å—à–µ, —á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å –≤—Ä–µ–º—è –≤–ø—É—Å—Ç—É—é.\n",
    " - –ß—Ç–æ–±—ã –±—ã—Å—Ç—Ä–æ –æ—Ç–ª–∞–¥–∏—Ç—å –∫–æ–¥, –ø—Ä–æ–±—É–π—Ç–µ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –º–∞–ª–µ–Ω—å–∫–æ–π —á–∞—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ (—Å–∫–∞–∂–µ–º, 5-10 –∫–∞—Ä—Ç–∏–Ω–æ–∫ –ø—Ä–æ—Å—Ç–æ —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ –∫–æ–¥ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è). –ö–æ–≥–¥–∞ –≤—ã –ø–æ–Ω—è–ª–∏, —á—Ç–æ —Å–º–æ–≥–ª–∏ –≤—Å—ë –æ—Ç–¥–µ–±–∞–∂–∏—Ç—å, –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –æ–±—É—á–µ–Ω–∏—é –ø–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É\n",
    " - –ù–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—É—Å–∫ –¥–µ–ª–∞–π—Ç–µ —Ä–æ–≤–Ω–æ –æ–¥–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤ –º–æ–¥–µ–ª–∏/–∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏/–æ–ø—Ç–∏–º–∞–π–∑–µ—Ä–µ, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —á—Ç–æ –∏ –∫–∞–∫ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\n",
    " - –§–∏–∫—Å–∏—Ä—É–π—Ç–µ random seed.\n",
    " - –ù–∞—á–∏–Ω–∞–π—Ç–µ —Å –ø—Ä–æ—Å—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫ —Å–ª–æ–∂–Ω—ã–º. –û–±—É—á–µ–Ω–∏–µ –ª—ë–≥–∫–∏—Ö –º–æ–¥–µ–ª–µ–π —ç–∫–æ–Ω–æ–º–∏—Ç –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.\n",
    " - –°—Ç–∞–≤—å—Ç–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ learning rate. –£–º–µ–Ω—å—à–∞–π—Ç–µ –µ–≥–æ, –∫–æ–≥–¥–∞ –ª–æ—Å—Å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–µ—Ä–µ—Å—Ç–∞—ë—Ç —É–±—ã–≤–∞—Ç—å.\n",
    " - –°–æ–≤–µ—Ç—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU. –ï—Å–ª–∏ —É –≤–∞—Å –µ–≥–æ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ google colab. –ï—Å–ª–∏ –≤–∞–º –Ω–µ—É–¥–æ–±–Ω–æ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π –æ—Å–Ω–æ–≤–µ, –Ω–∞–ø–∏—à–∏—Ç–µ –∏ –æ—Ç–ª–∞–¥—å—Ç–µ –≤–µ—Å—å –∫–æ–¥ –ª–æ–∫–∞–ª—å–Ω–æ –Ω–∞ CPU, –∞ –∑–∞—Ç–µ–º –∑–∞–ø—É—Å—Ç–∏—Ç–µ —É–∂–µ –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–π –Ω–æ—É—Ç–±—É–∫ –≤ –∫–æ–ª–∞–±–µ. –ê–≤—Ç–æ—Ä—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞–Ω–∏—è –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç—Ä–µ–±—É–µ–º–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –∫–æ–ª–∞–±–µ –∑–∞ 15 –º–∏–Ω—É—Ç –æ–±—É—á–µ–Ω–∏—è.\n",
    " \n",
    "Good luck & have fun! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import abc\n",
    "import time\n",
    "import typing\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import PIL\n",
    "import tqdm\n",
    "import torch\n",
    "import wandb\n",
    "import numpy\n",
    "import pandas\n",
    "import torchscan\n",
    "import torchvision\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as torchdata\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(device)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    numpy.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "def fix_random():\n",
    "    return set_random_seed(RANDOM_STATE)\n",
    "fix_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(anonymous = \"allow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ú® –í–Ω–∏–º–∞–Ω–∏–µ ‚ú®**\n",
    "\n",
    "–í —ç—Ç–æ–º –¥–æ–º–∞—à–Ω–µ–º –∑–∞–¥–∞–Ω–∏–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É `pytorch_lightning`. –î–æ—Å—Ç—É–ø –∫ –µ–µ [–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏](https://lightning.ai/docs/pytorch/stable/) –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Å —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –†–§. –í—ã –º–æ–∂–µ—Ç–µ:\n",
    "\n",
    "1. –ü–æ–ª—É—á–∏—Ç—å –∫ –Ω–µ–π –¥–æ—Å—Ç—É–ø —Å –ø–æ–º–æ—â—å—é VPN.\n",
    "\n",
    "2. –°–æ–±—Ä–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ. –î–ª—è —ç—Ç–æ–≥–æ —Å–∫–ª–æ–Ω–∏—Ä—É–π—Ç–µ [github-—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π](https://github.com/Lightning-AI/lightning/tree/master), –∑–∞–ø—É—Å—Ç–∏—Ç–µ –≤ –Ω–µ–º —Ç–µ—Ä–º–∏–Ω–∞–ª (–Ω–∞ windows ‚Äì git bash) –∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–º–∞–Ω–¥—ã:\n",
    "\n",
    "```shell\n",
    "git submodule update --init --recursive\n",
    "make docs\n",
    "```\n",
    "–ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –æ—Ç–∫—Ä–æ–π—Ç–µ –ø–æ—è–≤–∏–≤—à–∏–π—Å—è —Ñ–∞–π–ª `docs/build/html/index.html`. –î–ª—è —Ä–∞–±–æ—Ç—ã –∫–æ–º–∞–Ω–¥ –≤ –≤–∞—à–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å `pip`. –ü–æ–ª–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è [–ø–æ —Å—Å—ã–ª–∫–µ](https://github.com/Lightning-AI/lightning/tree/master/docs).\n",
    "\n",
    "3. –ì—É–≥–ª–∏—Ç—å `<error message> pytorch lightning` –∏–ª–∏ `<how to do this> pytorch lightning`. Stack overflow –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –†–§ –≤—Å–µ –µ—â–µ –¥–æ—Å—Ç—É–ø–µ–Ω üòâ\n",
    "\n",
    "4. –ù–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è `pytorch_lightning` –∏ –Ω–∞–ø–∏—Å–∞—Ç—å —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏ —Å —Ñ—É–Ω–∫—Ü–∏–µ–π `fit` –∏–∑ [—Å–µ–º–∏–Ω–∞—Ä–∞ 4](https://github.com/hse-ds/iad-deep-learning/blob/master/2023/seminars/04.%20Optim%20%26%20Lightning/04_Optim%26Lightning_solution.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RytEDW0ylRVN"
   },
   "source": [
    "## –ó–∞–¥–∞–Ω–∏–µ 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HZECedTvepi"
   },
   "source": [
    "### –ß—Ç–æ –ø–æ–º–æ–∂–µ—Ç —Å–¥–µ–ª–∞—Ç—å –Ω–∞ 10 –∏–∑ 10 (–æ–¥–Ω–æ –∑–∞–¥–∞–Ω–∏–µ - 5 –±–∞–ª–ª–æ–≤)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOioHGEiveso"
   },
   "source": [
    "1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —ç–∫—Å–ø–µ—Ä–µ–º–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –Ω–∏–º–∏.\n",
    "2. –ü–æ–¥–±–æ—Ä learning rate. –ü—Ä–∏–º–µ—Ä –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ —Å–µ–º–∏–Ω–∞—Ä–∞ –∫–∞–∫ —ç—Ç–æ –¥–µ–ª–∞—Ç—å: [–ö–∞–∫ –Ω–∞–π—Ç–∏ lr](https://pytorch-lightning.readthedocs.io/en/1.4.5/advanced/lr_finder.html)\n",
    "\n",
    "```\n",
    "  trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=2, auto_lr_find=True) \n",
    "\n",
    "  trainer.tune(module, train_dataloader, eval_dataloader)\n",
    "\n",
    "  trainer.fit(module, train_dataloader, eval_dataloader))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "3. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö. [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (–ø–æ–ª–µ–∑–Ω–∞—è)](https://pytorch.org/vision/main/transforms.html), –∞ —Ç–∞–∫–∂–µ [–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ albumentation](https://towardsdatascience.com/getting-started-with-albumentation-winning-deep-learning-image-augmentation-technique-in-pytorch-47aaba0ee3f8)\n",
    "4. –ü–æ–¥–±–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏. \n",
    "5. –ú–æ–∂–Ω–æ –Ω–∞–ø–∏—Å–∞—Ç—å –º–æ–¥–µ–ª—å —Ä—É–∫–∞–º–∏ —Å–≤–æ—é –≤ YourNet, –∞ –º–æ–∂–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é —Å–µ—Ç–∫—É –∏–∑–≤–µ—Å—Ç–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏–∑ –º–æ–¥—É–ª—è torchvision.models. –û–¥–∏–Ω –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤ –∫–∞–∫ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å: \n",
    "\n",
    "  * `torchvision.models.resnet18(pretrained=False, num_classes=200).to(device)`\n",
    "  * –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –≤–æ–∑–º–æ–∂–Ω—ã–º –º–æ–¥–µ–ª—è–º –∏ –∫–∞–∫ –∏—Ö –º–æ–∂–Ω–æ –±—Ä–∞—Ç—å: [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (–ø–æ–ª–µ–∑–Ω–∞—è)](https://pytorch.org/vision/stable/models.html)\n",
    "6. –ü—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏, –ø—Ä–∏–º–µ—Ä [—Ç—ã–∫, –Ω–æ —Ç—É—Ç –∏ –≤ —Ü–µ–ª–æ–º –≥–∞–π–¥ –æ—Ç –∏ –¥–æ](https://www.pluralsight.com/guides/image-classification-with-pytorch)\n",
    "7. Model Checkpointing. –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ —Å–≤–æ–π –ø—Ä–æ–≥—Ä–µ—Å—Å (–º–æ–¥–µ–ª–∏), —á—Ç–æ–±—ã –∫–æ–≥–¥–∞ —á—Ç–æ-—Ç–æ –ø–æ–π–¥–µ—Ç –Ω–µ —Ç–∞–∫ –≤—ã —Å–º–æ–∂–µ—Ç–µ –Ω–∞—á–∞—Ç—å —Å —ç—Ç–æ–≥–æ –º–µ—Å—Ç–∞ –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Å–≤–æ–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞–ª–∏. \n",
    " * –ü—Ä–∏–º–µ—Ä –∫–∞–∫ –º–æ–∂–Ω–æ —Å wandb —Ç—É—Ç: [–°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –≤ wandb](https://docs.wandb.ai/guides/integrations/lightning)\n",
    " * –ü–æ –ø—Ä–æ—Å—Ç–æ–º—É –º–æ–∂–Ω–æ —Ç–∞–∫: [–°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏ –≤ pytorch –¥–æ–∫–∞](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYePsQgNRB-n"
   },
   "source": [
    "### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MysteriousDataset(torchdata.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            train: bool,\n",
    "            preload: bool = True,\n",
    "            precalculate_transform: bool = True,\n",
    "            transform: typing.Optional[transforms.Compose] = None\n",
    "        ):\n",
    "        self.name = \"train\" if train else \"val\"\n",
    "        self.dataset_src = \"./dataset/{}\".format(self.name)\n",
    "        self.dataset = torchvision.datasets.ImageFolder(self.dataset_src)\n",
    "        self.classes = self.dataset.classes\n",
    "        self.precalculated_transform = None\n",
    "        self.transform = None\n",
    "\n",
    "        if preload or precalculate_transform:\n",
    "            if precalculate_transform:\n",
    "                self.precalculated_transform = transform\n",
    "                self.transform = transform\n",
    "                transform = None\n",
    "            self.images, self.targets = self.load_all(\"Preload {}\".format(self.name))\n",
    "        self.transform = transform\n",
    "\n",
    "    def load_all(self, progress_bar: bool = False):\n",
    "        images = [ ]\n",
    "        targets = [ ]\n",
    "        for record in (tqdm.tqdm(self, desc = progress_bar) if progress_bar else self):\n",
    "            images.append(record[0])\n",
    "            targets.append(record[1])\n",
    "        try: return torch.stack(images), targets\n",
    "        except: return images, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if hasattr(self, 'images') and hasattr(self, 'targets'):\n",
    "            image, target = self.images[idx], self.targets[idx]\n",
    "        else:\n",
    "            image, target = self.dataset[idx]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, target\n",
    "    \n",
    "    def channel_stats(self):\n",
    "        images, _ = self.load_all()\n",
    "        return torch.mean(images, dim = [0, 2, 3]), torch.std(images, dim = [0, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ transforms.ToImage(), transforms.ToDtype(torch.float32, scale = True) ])\n",
    "\n",
    "no_preload = MysteriousDataset(True, transform = transform, preload = False, precalculate_transform = False)\n",
    "no_precalc = MysteriousDataset(True, transform = transform, preload = True, precalculate_transform = False)\n",
    "preload = MysteriousDataset(True, transform = transform, preload = True, precalculate_transform = True)\n",
    "torch_dataset = torchvision.datasets.ImageFolder(\"./dataset/train\", transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that it is actually faster to loop through the dataset when it is preloaded into RAM\n",
    "def test_loading(*args):\n",
    "    for dataset, desc in args:\n",
    "        loader = torchdata.DataLoader(dataset, batch_size = 256)\n",
    "        for item in tqdm.tqdm(loader, desc = desc):\n",
    "            pass\n",
    "\n",
    "test_loading(\n",
    "    (torch_dataset, 'torch_dataset'),\n",
    "    (no_preload, 'no_preload'),\n",
    "    (no_precalc, 'no_precalc'),\n",
    "    (preload, 'preload')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ü–æ—Å—á–∏—Ç–∞–µ–º –ø–æ–∫–∞–Ω–∞–ª—å–Ω—ã–µ —Å—Ä–µ–¥–Ω–∏–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = preload.channel_stats()\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del torch_dataset\n",
    "del no_preload\n",
    "del no_precalc\n",
    "del preload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –°–æ–∑–¥–∞–¥–∏–º –¥–∞—Ç–∞—Å–µ—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale = True),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_set = MysteriousDataset(train = True, preload = True, precalculate_transform = True, transform = transform)\n",
    "test_set = MysteriousDataset(train = False, preload = True, precalculate_transform = True, transform = transform)\n",
    "\n",
    "# Check\n",
    "print(len(train_set), len(test_set))\n",
    "\n",
    "# Just very simple sanity checks\n",
    "assert isinstance(train_set[0], tuple)\n",
    "assert len(train_set[0]) == 2\n",
    "assert isinstance(train_set[1][1], int)\n",
    "assert isinstance(train_set[1][0], torch.Tensor)\n",
    "assert train_set[1][0].shape == torch.Size([ 3, 64, 64 ])\n",
    "print(\"Dataset tests passed\")\n",
    "\n",
    "for images, targets in torchdata.DataLoader(train_set, batch_size = 256):\n",
    "    assert isinstance(images, torch.Tensor)\n",
    "    assert isinstance(targets, torch.Tensor)\n",
    "    assert images.shape == torch.Size([ 256, 3, 64, 64 ])\n",
    "    assert targets.shape == torch.Size([ 256 ])\n",
    "    print(\"DataLoader tests passed\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOuM0EEYj7Ml"
   },
   "source": [
    "### –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–æ—á–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "\n",
    "# Denormalization\n",
    "denormalize = transforms.Compose([\n",
    "    transforms.Normalize(mean = [ 0., 0., 0. ], std = 1 / std),\n",
    "    transforms.Normalize(mean = -mean, std = [ 1., 1., 1. ])\n",
    "])\n",
    "\n",
    "# Display some samples from each dataset\n",
    "def display_examples(dataset: MysteriousDataset, row: int):\n",
    "    train_loader = torchdata.DataLoader(dataset, batch_size = 10, shuffle = True)\n",
    "    for i, (image, label) in enumerate(zip(*next(iter(train_loader)))):\n",
    "        plt.subplot(3, 10, i + 10 * (row - 1) + 1)\n",
    "        plt.axis('off')\n",
    "        plt.title('{}'.format(label))\n",
    "        plt.imshow((denormalize(image).permute(1, 2, 0).numpy() * 255).astype(numpy.uint8))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 5)\n",
    "display_examples(train_set, 1)\n",
    "display_examples(test_set, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCwKB-3nKm1-"
   },
   "source": [
    "## –ó–∞–¥–∞–Ω–∏–µ 1. \n",
    "\n",
    "5 –±–∞–ª–ª–æ–≤\n",
    "–î–æ–±–µ–π—Ç–µ—Å—å accuracy –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–µ –º–µ–Ω–µ–µ 0.44. –í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –∑–∞–ø—Ä–µ—â–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ —Ä–µ—Å–∞–π–∑–æ–º –∫–∞—Ä—Ç–∏–Ω–æ–∫.\n",
    "\n",
    "\n",
    "–î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –≤—ã–±–∏—Ç—å —Å–∫–æ—Ä (—Å—á–∏—Ç–∞–µ—Ç—Å—è –Ω–∏–∂–µ) –Ω–∞ 2.5/5 –±–∞–ª–ª–∞ (—Ç–æ –µ—Å—Ç—å –ø–æ–ª–æ–≤–∏–Ω—É –∑–∞ –∑–∞–¥–∞–Ω–∏–µ) –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–±–ª—é–¥–∞—Ç—å –ø–∞—Ä—É –ø—Ä–æ—Å—Ç—ã—Ö –∂–∏–∑–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª:\n",
    "1. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è (–±–µ–∑ –Ω–µ–µ —Å–ª–æ–∂–Ω–æ –æ—á–µ–Ω—å –±—É–¥–µ—Ç)\n",
    "2. –û–ø—Ç–∏–º–∞–π–∑–µ—Ä—ã –º–æ–∂–Ω–æ (–∏ –Ω—É–∂–Ω–æ) –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º. –û–¥–Ω–∞–∫–æ –∫–æ–≥–¥–∞ —á—Ç–æ-—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç–µ, —Ç–æ –Ω–µ –º–µ–Ω—è–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ä–∞–∑—É - —Å–æ–±—å–µ—Ç–µ –ª–æ–≥–∏–∫—É —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "3. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ —Å–∞–º—ã–µ –ø–µ—Ä–≤—ã–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª–µ–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (—á—Ç–æ –Ω–∞ –ª–µ–∫—Ü–∏—è—Ö –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å)\n",
    "4. –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ –Ω–æ—É—Ç–±—É–∫–∏ –ø—Ä–æ—à–µ–¥—à–∏—Ö —Å–µ–º–∏–Ω–∞—Ä–æ–≤ –∏ —Å–ª–µ–ø–∏—Ç—å –∏–∑ –Ω–∏—Ö —á—Ç–æ-—Ç–æ –æ–±—â–µ–µ. –°–µ–º–∏–Ω–∞—Ä—Å–∫–∏—Ö —Ç–µ—Ç—Ä–∞–¥–æ–∫ —Ö–≤–∞—Ç–∏—Ç —Å–≤–µ—Ä—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseClassifier(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, train_set: torchdata.Dataset, val_set: torchdata.Dataset, n_epochs: int = 25):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def predict(self, images: torch.Tensor) -> typing.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def calc_metrics(self, dataset: torchdata.Dataset) -> dict:\n",
    "        all_labels = torch.tensor([])\n",
    "        all_scores = torch.empty((0, len(dataset.classes)))\n",
    "        all_predictions = torch.tensor([])\n",
    "        loader = torchdata.DataLoader(dataset, batch_size = 256, shuffle = False)\n",
    "        for images, labels in loader:\n",
    "            all_labels = torch.cat([ all_labels, labels ])\n",
    "            predictions, scores = self.predict(images)\n",
    "            all_scores = torch.cat([ all_scores, scores.detach().cpu() ])\n",
    "            all_predictions = torch.cat([ all_predictions, predictions.detach().cpu() ])\n",
    "\n",
    "        return {\n",
    "            'Accuracy':       sklearn.metrics.accuracy_score      (all_labels, all_predictions),\n",
    "            'TOP-2 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 2),\n",
    "            'TOP-3 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 3),\n",
    "            'TOP-4 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 4),\n",
    "            'TOP-5 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 5),\n",
    "            'TOP-6 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 6),\n",
    "            'TOP-7 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 7),\n",
    "            'TOP-8 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 8),\n",
    "            'TOP-9 Accuracy': sklearn.metrics.top_k_accuracy_score(all_labels, all_scores, k = 9),\n",
    "            # 'AUC-ROC':        sklearn.metrics.roc_auc_score       (all_labels, all_scores, multi_class = 'ovo'),\n",
    "            'Precision':      sklearn.metrics.precision_score     (all_labels, all_predictions, average = 'macro', labels = numpy.unique(all_predictions)),\n",
    "            'Recall':         sklearn.metrics.recall_score        (all_labels, all_predictions, average = 'macro', labels = numpy.unique(all_predictions)),\n",
    "            'F1-score':       sklearn.metrics.f1_score            (all_labels, all_predictions, average = 'macro', labels = numpy.unique(all_predictions))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(BaseClassifier):\n",
    "    results = [ ]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            name: str,\n",
    "            model: torch.nn.Module,\n",
    "            batch_size: int = 256,\n",
    "            learning_rate: int = 1e-3,\n",
    "            device: torch.device = device,\n",
    "            optimizer: typing.Optional[torch.optim.Optimizer] = None,\n",
    "            scheduler: typing.Optional[torch.optim.lr_scheduler.LRScheduler] = None,\n",
    "        ):\n",
    "        self.name = name\n",
    "        self.history = [ ]\n",
    "        self.device = device\n",
    "        self.input_shape = None\n",
    "        self.scheduler = scheduler\n",
    "        self.batch_size = batch_size\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = optimizer or torch.optim.AdamW(self.model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "    def train(self, images: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "        self.model.train() # Enter train mode\n",
    "        self.optimizer.zero_grad() # Zero gradients\n",
    "        output = self.model(images.to(self.device)) # Get predictions\n",
    "        loss = torch.nn.functional.cross_entropy(output, labels.to(self.device)) # Calculate loss\n",
    "        loss.backward() # Calculate gradients\n",
    "        self.optimizer.step() # Update weights\n",
    "        return loss.item()\n",
    "\n",
    "    def train_epoch(self, loader: torchdata.DataLoader) -> float:\n",
    "        sum_loss = 0\n",
    "        for images, labels in loader:\n",
    "            sum_loss += self.train(images, labels) # Train one batch\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step()\n",
    "        return sum_loss / len(loader) # Return average loss to avoid random-dependent graph\n",
    "       \n",
    "    def fit(self, train_set: torchdata.Dataset, val_set: torchdata.Dataset, n_epochs: int = 25):\n",
    "        if self.input_shape is None:\n",
    "            self.predict(train_set[0][0].unsqueeze(0)) # Initialize lazy layers and input shape\n",
    "        loader = torchdata.DataLoader(train_set, batch_size = self.batch_size, shuffle = True)\n",
    "        wandb.init(project = \"DL-HW-2\", name = self.name, anonymous = \"allow\")\n",
    "        wandb.watch(self.model, log = \"all\")\n",
    "        for epoch in tqdm.trange(n_epochs, desc = 'Epochs'):\n",
    "            # Train\n",
    "            train_start = time.perf_counter()\n",
    "            loss = self.train_epoch(loader)\n",
    "            train_time = time.perf_counter() - train_start\n",
    "\n",
    "            # Validate\n",
    "            val_start = time.perf_counter()\n",
    "            metrics = self.calc_metrics(val_set)\n",
    "            val_time = time.perf_counter() - val_start\n",
    "            \n",
    "            # Upload metrics\n",
    "            metrics['Validation time'] = val_time\n",
    "            metrics['Train time'] = train_time\n",
    "            metrics['Loss'] = loss\n",
    "            wandb.log(metrics)\n",
    "            metrics['Epoch'] = epoch + 1\n",
    "            self.history.append(metrics)\n",
    "\n",
    "        # Finish the run\n",
    "        wandb.finish(quiet = True)\n",
    "\n",
    "        # Store best metrics\n",
    "        self.best_metrics = max(self.history, key = lambda item: item['Accuracy'])\n",
    "        Classifier.results.append({ 'Name': self.name, **self.best_metrics })\n",
    "\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def predict(self, images: torch.Tensor) -> typing.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if self.input_shape is None:\n",
    "            self.input_shape = images[0].shape # Lazily initialize input shape\n",
    "        self.model.eval() # Enter evaluation mode\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(images.to(self.device)) # Get outputs\n",
    "            scores = torch.softmax(outputs, dim = 1) # Make probabilities\n",
    "            predictions = torch.argmax(scores, dim = 1) # Calculate predictions\n",
    "        return predictions, scores\n",
    "    \n",
    "\n",
    "    def summary(self):\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        display(pandas.DataFrame(Classifier.results))\n",
    "        torchscan.summary(self.model.eval(), self.input_shape, receptive_field = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ò—Ç–æ–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(16), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(32), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(64), torch.nn.GELU(),\n",
    "    torch.nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(128), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "    torch.nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(256), torch.nn.GELU(),\n",
    "    torch.nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 1), torch.nn.BatchNorm2d(512), torch.nn.GELU(), torch.nn.MaxPool2d(2, 2),\n",
    "\n",
    "    torch.nn.Flatten(), torch.nn.Dropout(0.5), torch.nn.LazyLinear(1024), torch.nn.BatchNorm1d(1024), torch.nn.GELU(),\n",
    "    torch.nn.Dropout(0.5), torch.nn.Linear(1024, 200)\n",
    ")\n",
    "final_model = Classifier('First model', model).fit(train_set, test_set, 25)\n",
    "accuracy = final_model.calc_metrics(test_set)['Accuracy']\n",
    "print(f\"–û—Ü–µ–Ω–∫–∞ –∑–∞ —ç—Ç–æ –∑–∞–¥–∞–Ω–∏–µ —Å–æ—Å—Ç–∞–≤–∏—Ç {numpy.clip(10 * accuracy / 0.44, 0, 10):.2f} –±–∞–ª–ª–æ–≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZqSdlQQKukS"
   },
   "source": [
    "## –ó–∞–¥–∞–Ω–∏–µ 2\n",
    "\n",
    "5 –±–∞–ª–ª–æ–≤\n",
    "–î–æ–±–µ–π—Ç–µ—Å—å accuracy –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–µ –º–µ–Ω–µ–µ 0.84. –í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –¥–µ–ª–∞—Ç—å —Ä–µ—Å–∞–π–∑ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ—Ç—Ä–µ–π–Ω –º–æ–∂–Ω–æ.\n",
    "\n",
    "–î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –≤—ã–±–∏—Ç—å —Å–∫–æ—Ä (—Å—á–∏—Ç–∞–µ—Ç—Å—è –Ω–∏–∂–µ) –Ω–∞ 2.5/5 –±–∞–ª–ª–∞ (—Ç–æ –µ—Å—Ç—å –ø–æ–ª–æ–≤–∏–Ω—É –∑–∞ –∑–∞–¥–∞–Ω–∏–µ) –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–±–ª—é–¥–∞—Ç—å –ø–∞—Ä—É –ø—Ä–æ—Å—Ç—ã—Ö –∂–∏–∑–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª:\n",
    "1. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è (–±–µ–∑ –Ω–µ–µ —Å–ª–æ–∂–Ω–æ –æ—á–µ–Ω—å –±—É–¥–µ—Ç)\n",
    "2. –û–ø—Ç–∏–º–∞–π–∑–µ—Ä—ã –º–æ–∂–Ω–æ (–∏ –Ω—É–∂–Ω–æ) –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º. –û–¥–Ω–∞–∫–æ –∫–æ–≥–¥–∞ —á—Ç–æ-—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç–µ, —Ç–æ –Ω–µ –º–µ–Ω—è–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ä–∞–∑—É - —Å–æ–±—å–µ—Ç–µ –ª–æ–≥–∏–∫—É —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "3. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ —Å–∞–º—ã–µ –ø–µ—Ä–≤—ã–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª–µ–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (—á—Ç–æ –Ω–∞ –ª–µ–∫—Ü–∏—è—Ö –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –∏–ª–∏ –º–æ–∂–µ—Ç–µ –ø–æ–π—Ç–∏ –¥–∞–ª—å—à–µ).\n",
    "4. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–∞—á–∞–ª–∞ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –∫–∞–∫ baseline. –û—Ç—Å—é–¥–∞ –ø–æ–π–º–µ—Ç–µ –∫–∞–∫–∏–µ —Å–ª–æ–∏ –Ω—É–∂–Ω–æ –¥–æ–æ–±—É—á–∞—Ç—å.\n",
    "5. –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ –Ω–æ—É—Ç–±—É–∫–∏ –ø—Ä–æ—à–µ–¥—à–∏—Ö —Å–µ–º–∏–Ω–∞—Ä–æ–≤ –∏ —Å–ª–µ–ø–∏—Ç—å –∏–∑ –Ω–∏—Ö —á—Ç–æ-—Ç–æ –æ–±—â–µ–µ. –°–µ–º–∏–Ω–∞—Ä—Å–∫–∏—Ö —Ç–µ—Ç—Ä–∞–¥–æ–∫ —Ö–≤–∞—Ç–∏—Ç —Å–≤–µ—Ä—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feels like I can achieve needed quality without augmentation, so let's just extract and store features of all images beforehand\n",
    "class FeaturesDataset(torchdata.StackDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset: torchdata.Dataset,\n",
    "            extractor_weights: torchvision.models.WeightsEnum,\n",
    "            extractor_builder: typing.Callable[[], torch.nn.Module],\n",
    "            batch_size: int = 256,\n",
    "            extractor_device: torch.device = device\n",
    "        ):\n",
    "        self.name = 'Features for {}'.format(dataset.name if 'name' in dataset else 'undefined')\n",
    "\n",
    "        # If it is already a dataset of features, return\n",
    "        if isinstance(dataset, FeaturesDataset):\n",
    "            return super().__init__(dataset)\n",
    "\n",
    "        # https://github.com/pytorch/vision/issues/7744\n",
    "        def get_state_dict(self, *args, **kwargs):\n",
    "            kwargs.pop(\"check_hash\")\n",
    "            return torch.hub.load_state_dict_from_url(self.url, *args, **kwargs)\n",
    "        torchvision.models._api.WeightsEnum.get_state_dict = get_state_dict\n",
    "\n",
    "        # Load a pretrained model\n",
    "        self.extractor_device = extractor_device\n",
    "        self.extractor_weights = extractor_weights\n",
    "        self.extractor = extractor_builder(weights = self.extractor_weights)\n",
    "        last_layer = list(self.extractor.named_children())[-1]\n",
    "        self.extractor[last_layer[0]] = torch.nn.Identity()\n",
    "        self.extractor.to(self.extractor_device).eval()\n",
    "        print(last_layer)\n",
    "        \n",
    "        save_transform = dataset.transform if 'transform' in dataset else None\n",
    "        dataset.transform = None\n",
    "        assert isinstance(dataset[0][0], PIL.Image) # Without transsforms it should return PIL.Image\n",
    "        dataset.transform = self.extractor_weights.transforms() # Set transforms from pretrained model\n",
    "\n",
    "        with torch.no_grad():\n",
    "            targets = [ ]\n",
    "            features = [ ]\n",
    "            loader = torchdata.DataLoader(dataset, batch_size = batch_size)\n",
    "            for images_batch, targets_batch in tqdm.tqdm(loader, desc = self.name):\n",
    "                features_batch = self.extractor(images_batch.to(self.extractor_device))\n",
    "                features.append(features_batch.to('cpu'))\n",
    "                targets.append(targets_batch)\n",
    "\n",
    "        dataset.transform = save_transform # Restore transforms of the base dataset\n",
    "        super().__init__(torch.cat(features), torch.cat(targets)) # Initialize StackDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ò—Ç–æ–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random()\n",
    "builder = torchvision.models.resnext101_64x4d\n",
    "weights = torchvision.models.ResNeXt101_64X4D_Weights.IMAGENET1K_V1\n",
    "train_features = FeaturesDataset(train_set, weights, builder)\n",
    "test_features = FeaturesDataset(test_set, weights, builder)\n",
    "\n",
    "model = torch.nn.Sequential(torch.nn.Linear(2048, 1024), torch.nn.GELU(), torch.nn.Linear(1024, 200))\n",
    "final_model = Classifier('resnext101_64x4d', model, learning_rate = 3e-5).fit(train_features, test_features, 25)\n",
    "accuracy = final_model.calc_metrics(test_features)['Accuracy']\n",
    "print(f\"–û—Ü–µ–Ω–∫–∞ –∑–∞ —ç—Ç–æ –∑–∞–¥–∞–Ω–∏–µ —Å–æ—Å—Ç–∞–≤–∏—Ç {numpy.clip(10 * (accuracy - 0.5) / 0.34, 0, 10):.2f} –±–∞–ª–ª–æ–≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 15,
    "id": "pT8vfPSolRVb"
   },
   "source": [
    "# –û—Ç—á—ë—Ç –æ–± —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö \n",
    "\n",
    "—Ç–µ–∫—Å—Ç –ø–∏—Å–∞—Ç—å —Ç—É—Ç (–∏–ª–∏ —Å—Å—ã–ª–æ—á–∫—É –Ω–∞ wandb/–ª—é–±–æ–π —Ç—Ä–µ–∫–µ—Ä —ç–∫—Å–ø—Ä–µ–∏–º–µ–Ω—Ç–æ–≤) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è, —Ç–æ –µ—Å—Ç—å –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏–º–µ–Ω–Ω–æ —Ç—É—Ç —Ä–∏—Å–æ–≤–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏, –µ—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –≥–æ—Ç–æ–≤—ã–µ —Ç—Ä–µ–∫–µ—Ä—ã/–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∏ –≤–∞—à–∏—Ö –º–æ–¥–µ–ª–µ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "max_cell_id": 35
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
